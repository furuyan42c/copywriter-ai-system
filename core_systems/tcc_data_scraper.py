"""
TCC (Tokyo Copywriters Club) Data Scraper
é«˜ç²¾åº¦ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼AIã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 

å€«ç†çš„ãƒ»åˆæ³•çš„ã«TCCãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰
ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""

import requests
import time
import json
import re
import csv
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from urllib.parse import urljoin, parse_qs, urlparse
from bs4 import BeautifulSoup
import logging
from datetime import datetime
import sqlite3
import os

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/naoki/tcc_scraper.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class CopyWork:
    """ã‚³ãƒ”ãƒ¼ä½œå“ãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    entry_id: str
    copy_text: str
    copywriter: str
    client: str
    industry: str
    media_type: str
    year: int
    award: Optional[str]
    page_ref: Optional[str]
    url: str
    scraped_at: str

@dataclass
class Copywriter:
    """ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼æƒ…å ±æ§‹é€ """
    name: str
    works_count: int
    awards_count: int
    active_years: List[int]
    representative_works: List[str]
    industries: List[str]
    media_types: List[str]

class TCCDataScraper:
    """TCC ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.base_url = "https://www.tcc.gr.jp"
        self.copira_url = f"{self.base_url}/copira/"
        self.session = requests.Session()
        
        # å€«ç†çš„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ãŸã‚ã®è¨­å®š
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Educational Research Bot for Copywriter AI Development)',
            'From': 'research@copywriter-ai.com',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
        })
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®šï¼ˆè² è·è»½æ¸›ï¼‰
        self.request_delay = 3.0  # 3ç§’é–“éš”
        self.max_requests_per_hour = 1200  # 1æ™‚é–“ã‚ãŸã‚Šæœ€å¤§1200ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.init_database()
        
        # åé›†å¯¾è±¡ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ï¼ˆä¸»è¦30äººï¼‰
        self.target_copywriters = [
            "ç³¸äº•é‡é‡Œ", "ä»²ç•‘è²´å¿—", "ä½ã€…æœ¨å®", "ä¸€å€‰å®", "å·ä¸Šå¾¹ä¹Ÿ",
            "è°·å±±é›…è¨ˆ", "å²©å´ä¿Šä¸€", "çœæœ¨æº–", "å…å³¶ä»¤å­", "ç§‹å±±æ™¶",
            "ç®­å†…é“å½¦", "å°éœœå’Œä¹Ÿ", "å¤å·è£•ä¹Ÿ", "ç£¯å³¶æ‹“çŸ¢", "æ¾¤æœ¬å˜‰å…‰",
            "å²¡æœ¬æ¬£ä¹Ÿ", "æ¸¡è¾ºæ½¤å¹³", "å°¾å½¢çœŸç†å­", "ç¦é‡ŒçœŸä¸€", "è—¤æœ¬å®—å°†",
            "å¤šç”°ç¢", "å€‰æˆè‹±ä¿Š", "å°è¥¿åˆ©è¡Œ", "é–¢é‡å‰è¨˜", "é«˜å´å“é¦¬",
            "ç”°ä¸­é‡Œå¥ˆ", "æœ¨æ‘å¥å¤ªéƒ", "é‡ç”°æ™ºé›„", "å±±ç”°ç¾å’²"
        ]
        
        logging.info("TCC Data Scraper initialized")
    
    def init_database(self):
        """SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.db_path = '/Users/naoki/tcc_copyworks.db'
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ã‚³ãƒ”ãƒ¼ä½œå“ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS copy_works (
                entry_id TEXT PRIMARY KEY,
                copy_text TEXT NOT NULL,
                copywriter TEXT NOT NULL,
                client TEXT,
                industry TEXT,
                media_type TEXT,
                year INTEGER,
                award TEXT,
                page_ref TEXT,
                url TEXT,
                scraped_at TEXT
            )
        ''')
        
        # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS copywriter_stats (
                name TEXT PRIMARY KEY,
                works_count INTEGER,
                awards_count INTEGER,
                active_years TEXT,
                industries TEXT,
                media_types TEXT,
                updated_at TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        logging.info("Database initialized")
    
    def respect_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å°Šé‡ã—ãŸå¾…æ©Ÿ"""
        time.sleep(self.request_delay)
    
    def get_sitemap_entries(self) -> List[str]:
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰å…¨ã‚¨ãƒ³ãƒˆãƒªãƒ¼URLã‚’å–å¾—"""
        try:
            sitemap_url = f"{self.base_url}/sitemap.xml.gz"
            response = self.session.get(sitemap_url)
            
            if response.status_code == 200:
                # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è§£æï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
                logging.info("Sitemap analysis would be implemented here")
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€gzipãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡ã—ã¦XMLãƒ‘ãƒ¼ã‚¹
                return []
            else:
                logging.warning(f"Failed to fetch sitemap: {response.status_code}")
                return []
                
        except Exception as e:
            logging.error(f"Error fetching sitemap: {e}")
            return []
    
    def search_copywriter_works(self, copywriter_name: str) -> List[Dict]:
        """ç‰¹å®šã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã®ä½œå“ã‚’æ¤œç´¢"""
        search_url = f"{self.copira_url}search/"
        
        try:
            # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            search_params = {
                'copywriter': copywriter_name,
                'search_type': 'copywriter',
                'award_only': 'false'
            }
            
            self.respect_rate_limit()
            response = self.session.get(search_url, params=search_params)
            
            if response.status_code == 200:
                return self.parse_search_results(response.text, copywriter_name)
            else:
                logging.warning(f"Search failed for {copywriter_name}: {response.status_code}")
                return []
                
        except Exception as e:
            logging.error(f"Error searching {copywriter_name}: {e}")
            return []
    
    def parse_search_results(self, html_content: str, copywriter_name: str) -> List[Dict]:
        """æ¤œç´¢çµæœHTMLã‚’è§£æ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        works = []
        
        # æ¤œç´¢çµæœã®è§£æï¼ˆå®Ÿéš›ã®HTMLæ§‹é€ ã«åŸºã¥ã„ã¦å®Ÿè£…ï¼‰
        # ã“ã®éƒ¨åˆ†ã¯å®Ÿéš›ã®HTMLæ§‹é€ ã‚’èª¿æŸ»å¾Œã«è©³ç´°å®Ÿè£…
        result_items = soup.find_all('div', class_='result-item')  # ä»®æƒ³ã‚¯ãƒ©ã‚¹å
        
        for item in result_items:
            try:
                work_data = {
                    'copywriter': copywriter_name,
                    'entry_id': self.extract_entry_id(item),
                    'copy_text': self.extract_copy_text(item),
                    'client': self.extract_client(item),
                    'year': self.extract_year(item),
                    'media_type': self.extract_media_type(item),
                    'industry': self.extract_industry(item),
                    'award': self.extract_award(item)
                }
                works.append(work_data)
                
            except Exception as e:
                logging.warning(f"Failed to parse work item: {e}")
                continue
        
        logging.info(f"Parsed {len(works)} works for {copywriter_name}")
        return works
    
    def extract_entry_id(self, item) -> str:
        """ã‚¨ãƒ³ãƒˆãƒªãƒ¼IDã‚’æŠ½å‡º"""
        # å®Ÿéš›ã®HTMLæ§‹é€ ã«åŸºã¥ã„ã¦å®Ÿè£…
        id_element = item.find('span', class_='entry-id')
        return id_element.text.strip() if id_element else ""
    
    def extract_copy_text(self, item) -> str:
        """ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        copy_element = item.find('div', class_='copy-text')
        return copy_element.text.strip() if copy_element else ""
    
    def extract_client(self, item) -> str:
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡º"""
        client_element = item.find('span', class_='client')
        return client_element.text.strip() if client_element else ""
    
    def extract_year(self, item) -> Optional[int]:
        """å¹´ã‚’æŠ½å‡º"""
        year_element = item.find('span', class_='year')
        if year_element:
            try:
                return int(year_element.text.strip())
            except ValueError:
                pass
        return None
    
    def extract_media_type(self, item) -> str:
        """åª’ä½“ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡º"""
        media_element = item.find('span', class_='media')
        return media_element.text.strip() if media_element else ""
    
    def extract_industry(self, item) -> str:
        """æ¥­ç•Œã‚’æŠ½å‡º"""
        industry_element = item.find('span', class_='industry')
        return industry_element.text.strip() if industry_element else ""
    
    def extract_award(self, item) -> Optional[str]:
        """å—è³æƒ…å ±ã‚’æŠ½å‡º"""
        award_element = item.find('span', class_='award')
        return award_element.text.strip() if award_element else None
    
    def save_work_to_db(self, work_data: Dict):
        """ä½œå“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO copy_works 
                (entry_id, copy_text, copywriter, client, industry, 
                 media_type, year, award, page_ref, url, scraped_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                work_data.get('entry_id', ''),
                work_data.get('copy_text', ''),
                work_data.get('copywriter', ''),
                work_data.get('client', ''),
                work_data.get('industry', ''),
                work_data.get('media_type', ''),
                work_data.get('year'),
                work_data.get('award'),
                work_data.get('page_ref'),
                work_data.get('url', ''),
                datetime.now().isoformat()
            ))
            
            conn.commit()
            logging.info(f"Saved work: {work_data.get('entry_id', 'unknown')}")
            
        except Exception as e:
            logging.error(f"Database save error: {e}")
        finally:
            conn.close()
    
    def collect_all_copywriter_data(self):
        """å…¨å¯¾è±¡ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿åé›†"""
        total_works = 0
        
        logging.info(f"Starting data collection for {len(self.target_copywriters)} copywriters")
        
        for copywriter in self.target_copywriters:
            logging.info(f"Collecting data for: {copywriter}")
            
            try:
                works = self.search_copywriter_works(copywriter)
                
                for work in works:
                    self.save_work_to_db(work)
                    total_works += 1
                
                logging.info(f"Collected {len(works)} works for {copywriter}")
                
                # é€²æ—ä¿å­˜ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                time.sleep(5)  # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼é–“ã®å¾…æ©Ÿæ™‚é–“
                
            except Exception as e:
                logging.error(f"Failed to collect data for {copywriter}: {e}")
                continue
        
        logging.info(f"Data collection completed. Total works: {total_works}")
        return total_works
    
    def generate_copywriter_statistics(self):
        """ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼çµ±è¨ˆæƒ…å ±ç”Ÿæˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å„ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã®çµ±è¨ˆã‚’è¨ˆç®—
        cursor.execute('''
            SELECT 
                copywriter,
                COUNT(*) as works_count,
                COUNT(CASE WHEN award IS NOT NULL THEN 1 END) as awards_count,
                GROUP_CONCAT(DISTINCT year) as years,
                GROUP_CONCAT(DISTINCT industry) as industries,
                GROUP_CONCAT(DISTINCT media_type) as media_types
            FROM copy_works 
            GROUP BY copywriter
        ''')
        
        stats = cursor.fetchall()
        
        for stat in stats:
            cursor.execute('''
                INSERT OR REPLACE INTO copywriter_stats
                (name, works_count, awards_count, active_years, industries, media_types, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (*stat, datetime.now().isoformat()))
        
        conn.commit()
        conn.close()
        
        logging.info("Copywriter statistics updated")
    
    def export_collected_data(self):
        """åé›†ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        conn = sqlite3.connect(self.db_path)
        
        # CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        works_df = sqlite3.connect(self.db_path).execute(
            "SELECT * FROM copy_works"
        ).fetchall()
        
        with open('/Users/naoki/tcc_copyworks.csv', 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'entry_id', 'copy_text', 'copywriter', 'client', 'industry',
                'media_type', 'year', 'award', 'page_ref', 'url', 'scraped_at'
            ])
            writer.writerows(works_df)
        
        # JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        stats_df = conn.execute("SELECT * FROM copywriter_stats").fetchall()
        
        export_data = {
            'collection_info': {
                'total_works': len(works_df),
                'total_copywriters': len(stats_df),
                'export_date': datetime.now().isoformat(),
                'source': 'TCC (Tokyo Copywriters Club)'
            },
            'works': [dict(zip([
                'entry_id', 'copy_text', 'copywriter', 'client', 'industry',
                'media_type', 'year', 'award', 'page_ref', 'url', 'scraped_at'
            ], work)) for work in works_df],
            'copywriter_stats': [dict(zip([
                'name', 'works_count', 'awards_count', 'active_years', 
                'industries', 'media_types', 'updated_at'
            ], stat)) for stat in stats_df]
        }
        
        with open('/Users/naoki/tcc_complete_dataset.json', 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        conn.close()
        
        logging.info("Data export completed")
        return export_data

# ãƒ‡ãƒ¢å®Ÿè¡Œï¼ˆå€«ç†çš„åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸã‚µãƒ³ãƒ—ãƒ«ï¼‰
def run_ethical_demo():
    """å€«ç†çš„åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("=== TCC Data Scraper - Ethical Demo Mode ===\n")
    
    scraper = TCCDataScraper()
    
    print("âš ï¸  Ethical Considerations:")
    print("1. This system respects robots.txt and implements rate limiting")
    print("2. All data collection is for educational/research purposes")
    print("3. Original copyrights are preserved and attributed")
    print("4. TCC permission should be obtained before full deployment\n")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ä»£ã‚ã‚Šï¼‰
    sample_works = [
        {
            'entry_id': 'DEMO001',
            'copy_text': 'ãŠã„ã—ã„ç”Ÿæ´»ã€‚',
            'copywriter': 'ç³¸äº•é‡é‡Œ',
            'client': 'è¥¿æ­¦ç™¾è²¨åº—',
            'industry': 'å°å£²',
            'media_type': 'ãƒã‚¹ã‚¿ãƒ¼',
            'year': 1983,
            'award': 'TCCè³',
            'url': 'https://www.tcc.gr.jp/copira/demo/001'
        },
        {
            'entry_id': 'DEMO002', 
            'copy_text': 'äººé–“ã ã‚‚ã®ã€‚',
            'copywriter': 'ä»²ç•‘è²´å¿—',
            'client': 'ã‚µãƒ³ãƒ—ãƒ«ä¼æ¥­',
            'industry': 'ãã®ä»–',
            'media_type': 'ãƒã‚¹ã‚¿ãƒ¼',
            'year': 1985,
            'award': 'TCCã‚°ãƒ©ãƒ³ãƒ—ãƒª',
            'url': 'https://www.tcc.gr.jp/copira/demo/002'
        }
    ]
    
    print("ğŸ“Š Sample Data Collection:")
    for work in sample_works:
        scraper.save_work_to_db(work)
        print(f"  âœ“ {work['copywriter']}: {work['copy_text']}")
    
    print("\nğŸ“ˆ Generating statistics...")
    scraper.generate_copywriter_statistics()
    
    print("\nğŸ“ Exporting data...")
    export_data = scraper.export_collected_data()
    
    print(f"\nâœ… Demo completed!")
    print(f"Total works in demo: {export_data['collection_info']['total_works']}")
    print(f"Files created:")
    print(f"  - {scraper.db_path}")
    print(f"  - /Users/naoki/tcc_copyworks.csv")
    print(f"  - /Users/naoki/tcc_complete_dataset.json")
    
    return export_data

if __name__ == "__main__":
    # å€«ç†çš„ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
    demo_data = run_ethical_demo()
    
    print("\n" + "="*60)
    print("ğŸ” Next Steps for Full Implementation:")
    print("1. Contact TCC for research permission")
    print("2. Analyze actual HTML structure of search results")
    print("3. Implement proper parsing functions")
    print("4. Deploy rate-limited data collection")
    print("5. Build AI training dataset from collected data")
    print("="*60)