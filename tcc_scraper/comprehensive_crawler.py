#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

class ComprehensiveTCCCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://www.tcc.gr.jp/copira/"
        self.all_urls = set()
        self.all_data = []
        self.processed = 0
        self.failed = 0
        self.lock = threading.Lock()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs("tcc_comprehensive_data", exist_ok=True)
        
        print("ğŸš€ Comprehensive TCC Crawler Starting...")
        print(f"ğŸ“… Start time: {datetime.now().strftime('%H:%M:%S')}")
        print("ğŸ¯ Goal: Complete collection of all 37,259+ items")
        print("=" * 60)
    
    def get_page(self, url, timeout=20):
        """å …ç‰¢ãªãƒšãƒ¼ã‚¸å–å¾—"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âš ï¸ Error fetching {url}: {e}")
            return None
    
    def investigate_search_structure(self):
        """æ¤œç´¢æ§‹é€ ã‚’è©³ç´°èª¿æŸ»"""
        print("ğŸ” Investigating search structure...")
        
        # 1. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¤œç´¢ã§ã®ç·ä»¶æ•°ã‚’ç¢ºèª
        default_url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all"
        html = self.get_page(default_url)
        
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç·ä»¶æ•°ã‚’æ¢ã™
            page_text = soup.get_text()
            
            # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç·ä»¶æ•°ã‚’æ¤œç´¢
            patterns = [
                r'(\d{1,3}(?:,\d{3})*)ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ',
                r'(\d{1,3}(?:,\d{3})*)ä»¶ã®æ¤œç´¢çµæœ',
                r'å…¨(\d{1,3}(?:,\d{3})*)ä»¶',
                r'(\d{1,3}(?:,\d{3})*)ä½œå“'
            ]
            
            total_items = None
            for pattern in patterns:
                match = re.search(pattern, page_text)
                if match:
                    total_str = match.group(1).replace(',', '')
                    total_items = int(total_str)
                    print(f"ğŸ“Š Found total items: {total_items:,} (pattern: {pattern})")
                    break
            
            if not total_items:
                print("âš ï¸ Could not find total item count in text")
                # HTMLã‹ã‚‰ä»¶æ•°ã‚’æ¨å®š
                print("ğŸ” Analyzing page structure for estimation...")
                
                # ãƒšãƒ¼ã‚¸å†…ã®ãƒªãƒ³ã‚¯ã‹ã‚‰æ¨å®š
                detail_links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                print(f"ğŸ“„ Detail links on this page: {len(detail_links)}")
                
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¨å®š
                pagination_links = soup.find_all('a', href=True)
                page_numbers = []
                for link in pagination_links:
                    text = link.get_text(strip=True)
                    if text.isdigit():
                        page_numbers.append(int(text))
                
                if page_numbers:
                    max_page = max(page_numbers)
                    estimated_total = max_page * len(detail_links) if detail_links else max_page * 20
                    print(f"ğŸ“„ Max page found: {max_page}, Estimated total: {estimated_total:,}")
                    total_items = estimated_total
                else:
                    print("âš ï¸ No pagination found, using conservative estimate")
                    total_items = 40000  # ä¿å®ˆçš„ãªæ¨å®š
        
        return total_items or 37259  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
    
    def strategy_year_by_year_search(self):
        """å¹´åº¦åˆ¥æ¤œç´¢æˆ¦ç•¥"""
        print("ğŸ—“ï¸ Starting year-by-year comprehensive search...")
        all_urls = set()
        
        # 1960å¹´ã‹ã‚‰2025å¹´ã¾ã§å„å¹´åº¦ã‚’æ¤œç´¢
        for year in range(1960, 2026):
            print(f"ğŸ“… Searching year {year}...", end=' ')
            
            year_urls = set()
            
            # ãã®å¹´åº¦ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            base_url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start={year}&end={year}&target_prize=all"
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
            page = 1
            consecutive_empty = 0
            max_consecutive_empty = 3
            
            while consecutive_empty < max_consecutive_empty:
                page_url = f"{base_url}&page={page}"
                html = self.get_page(page_url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # è©³ç´°URLã‚’æŠ½å‡º
                    detail_links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                    page_found = 0
                    
                    for link in detail_links:
                        href = link.get('href', '')
                        if not href.startswith('http'):
                            href = f"https://www.tcc.gr.jp{href}"
                        year_urls.add(href)
                        page_found += 1
                    
                    if page_found > 0:
                        consecutive_empty = 0
                    else:
                        consecutive_empty += 1
                        
                    page += 1
                    time.sleep(0.2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    
                    # å®‰å…¨ä¸Šé™
                    if page > 500:
                        break
                else:
                    consecutive_empty += 1
            
            all_urls.update(year_urls)
            print(f"Found {len(year_urls)} URLs (Total: {len(all_urls):,})")
            
            # å®šæœŸä¿å­˜
            if year % 10 == 0:
                self.save_urls_checkpoint(all_urls, f"year_{year}")
        
        return all_urls
    
    def strategy_award_category_search(self):
        """å—è³ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¤œç´¢æˆ¦ç•¥"""
        print("ğŸ† Starting award category search...")
        all_urls = set()
        
        # å—è³ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ¤œç´¢
        award_categories = [
            'all',  # å…¨ã¦ã®è³
            'tcc',  # TCCè³
            'new',  # æ–°äººè³
            'student'  # å­¦ç”Ÿè³
        ]
        
        for category in award_categories:
            print(f"ğŸ† Searching award category: {category}...", end=' ')
            
            category_urls = set()
            base_url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize={category}"
            
            page = 1
            consecutive_empty = 0
            
            while consecutive_empty < 5:
                page_url = f"{base_url}&page={page}"
                html = self.get_page(page_url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    detail_links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                    
                    page_found = 0
                    for link in detail_links:
                        href = link.get('href', '')
                        if not href.startswith('http'):
                            href = f"https://www.tcc.gr.jp{href}"
                        category_urls.add(href)
                        page_found += 1
                    
                    if page_found > 0:
                        consecutive_empty = 0
                    else:
                        consecutive_empty += 1
                        
                    page += 1
                    time.sleep(0.2)
                    
                    if page > 1000:
                        break
                else:
                    consecutive_empty += 1
            
            all_urls.update(category_urls)
            print(f"Found {len(category_urls)} URLs (Total: {len(all_urls):,})")
        
        return all_urls
    
    def strategy_media_type_search(self):
        """åª’ä½“åˆ¥æ¤œç´¢æˆ¦ç•¥"""
        print("ğŸ“º Starting media type search...")
        all_urls = set()
        
        # åª’ä½“ã‚¿ã‚¤ãƒ—åˆ¥æ¤œç´¢
        media_types = [
            'tv',      # ãƒ†ãƒ¬ãƒ“
            'radio',   # ãƒ©ã‚¸ã‚ª
            'print',   # å°åˆ·
            'web',     # ã‚¦ã‚§ãƒ–
            'outdoor', # å±‹å¤–
            'other'    # ãã®ä»–
        ]
        
        for media in media_types:
            print(f"ğŸ“º Searching media type: {media}...", end=' ')
            
            media_urls = set()
            base_url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media={media}&start=1960&end=2025&target_prize=all"
            
            page = 1
            consecutive_empty = 0
            
            while consecutive_empty < 5:
                page_url = f"{base_url}&page={page}"
                html = self.get_page(page_url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    detail_links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                    
                    page_found = 0
                    for link in detail_links:
                        href = link.get('href', '')
                        if not href.startswith('http'):
                            href = f"https://www.tcc.gr.jp{href}"
                        media_urls.add(href)
                        page_found += 1
                    
                    if page_found > 0:
                        consecutive_empty = 0
                    else:
                        consecutive_empty += 1
                        
                    page += 1
                    time.sleep(0.2)
                    
                    if page > 1000:
                        break
                else:
                    consecutive_empty += 1
            
            all_urls.update(media_urls)
            print(f"Found {len(media_urls)} URLs (Total: {len(all_urls):,})")
        
        return all_urls
    
    def strategy_direct_id_enumeration(self):
        """ç›´æ¥IDåˆ—æŒ™æˆ¦ç•¥"""
        print("ğŸ”¢ Starting direct ID enumeration...")
        all_urls = set()
        
        # æ—¢çŸ¥ã®IDãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        # 2023001, 2023352, 2024001 ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨æ¸¬
        
        id_ranges = [
            (1, 10000),           # åˆæœŸã®ID
            (2020001, 2020999),   # 2020å¹´
            (2021001, 2021999),   # 2021å¹´
            (2022001, 2022999),   # 2022å¹´
            (2023001, 2023999),   # 2023å¹´
            (2024001, 2024999),   # 2024å¹´
            (2025001, 2025999),   # 2025å¹´
        ]
        
        for start_id, end_id in id_ranges:
            print(f"ğŸ”¢ Checking ID range {start_id}-{end_id}...")
            
            range_urls = set()
            batch_size = 100
            
            for i in range(start_id, end_id + 1, batch_size):
                batch_end = min(i + batch_size - 1, end_id)
                
                # ãƒãƒƒãƒã§HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
                with ThreadPoolExecutor(max_workers=5) as executor:
                    futures = []
                    for id_num in range(i, batch_end + 1):
                        url = f"https://www.tcc.gr.jp/copira/id/{id_num}/"
                        future = executor.submit(self.check_url_exists, url)
                        futures.append((future, url))
                    
                    for future, url in futures:
                        if future.result():
                            range_urls.add(url)
                
                if i % 1000 == 0:
                    print(f"  Checked up to ID {i}, found {len(range_urls)} URLs")
                
                time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            all_urls.update(range_urls)
            print(f"ID range {start_id}-{end_id}: Found {len(range_urls)} URLs (Total: {len(all_urls):,})")
        
        return all_urls
    
    def check_url_exists(self, url):
        """URLã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆHEADãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰"""
        try:
            response = self.session.head(url, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def save_urls_checkpoint(self, urls, suffix=""):
        """URLåé›†ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        filename = f"tcc_comprehensive_data/urls_checkpoint_{suffix}_{datetime.now().strftime('%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(list(urls), f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ URLs checkpoint saved: {filename}")
    
    def comprehensive_url_collection(self):
        """åŒ…æ‹¬çš„URLåé›†"""
        print("ğŸŒ Starting comprehensive URL collection...")
        
        # èª¿æŸ»
        estimated_total = self.investigate_search_structure()
        print(f"ğŸ¯ Target: {estimated_total:,} items")
        
        # è¤‡æ•°æˆ¦ç•¥ã‚’å®Ÿè¡Œ
        strategies = [
            ("Year-by-year search", self.strategy_year_by_year_search),
            ("Award category search", self.strategy_award_category_search),
            ("Media type search", self.strategy_media_type_search),
            # ("Direct ID enumeration", self.strategy_direct_id_enumeration),  # æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚æœ€å¾Œã«
        ]
        
        for strategy_name, strategy_func in strategies:
            print(f"\nğŸ”„ Executing: {strategy_name}")
            try:
                strategy_urls = strategy_func()
                new_urls = strategy_urls - self.all_urls
                self.all_urls.update(new_urls)
                print(f"âœ… {strategy_name}: +{len(new_urls):,} new URLs (Total: {len(self.all_urls):,})")
                
                # ä¸­é–“ä¿å­˜
                self.save_urls_checkpoint(self.all_urls, strategy_name.replace(' ', '_').lower())
                
            except Exception as e:
                print(f"âŒ {strategy_name} failed: {e}")
        
        print(f"\nğŸ‰ URL collection complete: {len(self.all_urls):,} total URLs")
        return list(self.all_urls)
    
    def parse_detail_page(self, url):
        """è©³ç´°ãƒšãƒ¼ã‚¸è§£æ"""
        html = self.get_page(url)
        if not html:
            return {'error': 'fetch_failed', 'url': url}
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data = {
                'url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            # IDæŠ½å‡º
            id_match = re.search(r'/id/(\d+)', url)
            if id_match:
                data['id'] = id_match.group(1)
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            title = soup.find('h1')
            if title:
                data['title'] = title.get_text(strip=True)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                        value = td.get_text(strip=True)
                        
                        # ã‚­ãƒ¼æ­£è¦åŒ–
                        key_map = {
                            'åºƒå‘Šä¸»': 'advertiser',
                            'å—è³': 'award',
                            'æ¥­ç¨®': 'industry',
                            'åª’ä½“': 'media_type',
                            'æ²è¼‰å¹´åº¦': 'publication_year',
                            'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                            'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                            'åºƒå‘Šä¼šç¤¾': 'agency',
                            'åˆ¶ä½œä¼šç¤¾': 'production_company'
                        }
                        
                        normalized_key = key_map.get(key, key.lower().replace(' ', '_'))
                        if value:
                            data[normalized_key] = value
            
            # å¹´åº¦ã‚’æ•°å€¤åŒ–
            if 'publication_year' in data:
                year_match = re.search(r'(\d{4})', data['publication_year'])
                if year_match:
                    data['year'] = int(year_match.group(1))
            
            return data
            
        except Exception as e:
            return {'error': str(e), 'url': url}
    
    def process_urls_parallel(self, urls):
        """ä¸¦åˆ—URLå‡¦ç†"""
        print(f"âš¡ Processing {len(urls):,} URLs in parallel...")
        
        start_time = datetime.now()
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            # ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
            future_to_url = {executor.submit(self.parse_detail_page, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    
                    with self.lock:
                        self.all_data.append(result)
                        
                        if 'error' not in result:
                            self.processed += 1
                        else:
                            self.failed += 1
                        
                        # é€²æ—è¡¨ç¤º
                        total_processed = len(self.all_data)
                        if total_processed % 100 == 0:
                            elapsed = datetime.now() - start_time
                            rate = total_processed / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
                            eta_seconds = (len(urls) - total_processed) / rate if rate > 0 else 0
                            eta_minutes = eta_seconds / 60
                            
                            success_rate = self.processed / total_processed * 100
                            print(f"ğŸ“Š {total_processed:,}/{len(urls):,} ({total_processed/len(urls)*100:.1f}%) - "
                                  f"Success: {success_rate:.1f}% - "
                                  f"Speed: {rate:.1f}/s - "
                                  f"ETA: {eta_minutes:.1f}min")
                            
                            # ãƒãƒƒãƒä¿å­˜
                            if total_processed % 500 == 0:
                                self.save_batch(total_processed)
                
                except Exception as e:
                    with self.lock:
                        self.failed += 1
                        print(f"âŒ Error processing {url}: {e}")
        
        print(f"âœ… Parallel processing complete!")
        print(f"   Success: {self.processed:,}")
        print(f"   Failed: {self.failed:,}")
        print(f"   Total time: {datetime.now() - start_time}")
    
    def save_batch(self, count):
        """ãƒãƒƒãƒä¿å­˜"""
        filename = f"tcc_comprehensive_data/batch_{count:06d}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Batch saved: {filename}")
    
    def save_final_comprehensive_data(self):
        """æœ€çµ‚åŒ…æ‹¬ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON
        json_file = f"tcc_comprehensive_data/tcc_comprehensive_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        
        # CSV
        csv_file = f"tcc_comprehensive_data/tcc_comprehensive_{timestamp}.csv"
        self.save_as_csv(csv_file)
        
        # çµ±è¨ˆ
        stats_file = f"tcc_comprehensive_data/tcc_comprehensive_stats_{timestamp}.txt"
        self.save_comprehensive_stats(stats_file)
        
        print(f"ğŸ’¾ Comprehensive data saved:")
        print(f"   ğŸ“‹ JSON: {json_file}")
        print(f"   ğŸ“Š CSV: {csv_file}")
        print(f"   ğŸ“ˆ Stats: {stats_file}")
        
        return json_file, csv_file, stats_file
    
    def save_as_csv(self, filename):
        """CSVä¿å­˜"""
        import csv
        
        valid_data = [item for item in self.all_data if 'error' not in item]
        if not valid_data:
            return
        
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åé›†
        all_fields = set()
        for item in valid_data:
            all_fields.update(item.keys())
        
        simple_fields = sorted([f for f in all_fields])
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=simple_fields)
            writer.writeheader()
            
            for item in valid_data:
                row = {}
                for field in simple_fields:
                    value = item.get(field, '')
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    row[field] = str(value)[:500]
                writer.writerow(row)
    
    def save_comprehensive_stats(self, filename):
        """åŒ…æ‹¬çµ±è¨ˆä¿å­˜"""
        valid_data = [item for item in self.all_data if 'error' not in item]
        
        stats = []
        stats.append("TCC ã‚³ãƒ”ãƒ© åŒ…æ‹¬ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµæœ")
        stats.append("=" * 60)
        stats.append(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        stats.append(f"ç·ä»¶æ•°: {len(self.all_data):,}")
        stats.append(f"æˆåŠŸ: {len(valid_data):,}")
        stats.append(f"å¤±æ•—: {len(self.all_data) - len(valid_data):,}")
        stats.append(f"æˆåŠŸç‡: {len(valid_data)/len(self.all_data)*100:.1f}%" if self.all_data else "N/A")
        stats.append("")
        
        # è©³ç´°çµ±è¨ˆ
        if valid_data:
            # å¹´åº¦çµ±è¨ˆ
            years = {}
            for item in valid_data:
                year = item.get('year', item.get('publication_year', 'Unknown'))
                years[year] = years.get(year, 0) + 1
            
            stats.append("å¹´åº¦åˆ¥çµ±è¨ˆ (ä¸Šä½30):")
            for year, count in sorted(years.items(), key=lambda x: x[0] if isinstance(x[0], int) else 0, reverse=True)[:30]:
                stats.append(f"  {year}: {count:,}ä»¶")
            
            # åª’ä½“çµ±è¨ˆ
            media = {}
            for item in valid_data:
                media_type = item.get('media_type', 'Unknown')
                media[media_type] = media.get(media_type, 0) + 1
            
            stats.append("\nåª’ä½“åˆ¥çµ±è¨ˆ:")
            for media_type, count in sorted(media.items(), key=lambda x: x[1], reverse=True):
                stats.append(f"  {media_type}: {count:,}ä»¶")
            
            # å—è³çµ±è¨ˆ
            awards = {}
            for item in valid_data:
                award = item.get('award', 'ãªã—')
                awards[award] = awards.get(award, 0) + 1
            
            stats.append("\nå—è³çµ±è¨ˆ (ä¸Šä½20):")
            for award, count in sorted(awards.items(), key=lambda x: x[1], reverse=True)[:20]:
                stats.append(f"  {award}: {count:,}ä»¶")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('\n'.join(stats))
    
    def run_comprehensive_crawl(self):
        """åŒ…æ‹¬ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: åŒ…æ‹¬çš„URLåé›†
            urls = self.comprehensive_url_collection()
            
            if not urls:
                print("âŒ No URLs collected!")
                return
            
            print(f"\nğŸ¯ Target URLs: {len(urls):,}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            self.process_urls_parallel(urls)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚åŒ…æ‹¬ä¿å­˜
            files = self.save_final_comprehensive_data()
            
            print("\nğŸ‰ Comprehensive crawl completed successfully!")
            print(f"ğŸ“Š Final stats: {self.processed:,} success, {self.failed:,} failed")
            print(f"ğŸ¯ Coverage: {len(self.all_data):,} / 37,259 ({len(self.all_data)/37259*100:.1f}%)")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ Interrupted by user")
            if self.all_data:
                self.save_final_comprehensive_data()
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            if self.all_data:
                self.save_final_comprehensive_data()

if __name__ == "__main__":
    crawler = ComprehensiveTCCCrawler()
    crawler.run_comprehensive_crawl()