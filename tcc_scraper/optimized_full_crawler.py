import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
import os
from urllib.parse import urljoin
import signal
import sys
import threading
from queue import Queue, Empty

class OptimizedTCCFullCrawler:
    def __init__(self, output_dir="tcc_complete_data", delay=0.5, batch_size=100):
        self.output_dir = output_dir
        self.delay = delay
        self.batch_size = batch_size
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
        })
        
        self.total_items = 0
        self.processed_items = 0
        self.failed_items = 0
        self.start_time = None
        self.stop_crawling = False
        
        # é€²æ—è¿½è·¡
        self.current_phase = "æº–å‚™ä¸­"
        self.phase_progress = 0
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¸­æ–­å‡¦ç†
        signal.signal(signal.SIGINT, self.signal_handler)
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        self.checkpoint_file = os.path.join(self.output_dir, "crawl_checkpoint.json")
        self.urls_file = os.path.join(self.output_dir, "all_urls.json")
        self.progress_file = os.path.join(self.output_dir, "progress.json")
    
    def signal_handler(self, signum, frame):
        print(f"\nâš ï¸ åœæ­¢ä¿¡å·ã‚’å—ä¿¡ã€‚å®‰å…¨ã«çµ‚äº†ä¸­...")
        self.stop_crawling = True
    
    def save_progress(self):
        """é€²æ—ã‚’ä¿å­˜"""
        progress_data = {
            'total_items': self.total_items,
            'processed_items': self.processed_items,
            'failed_items': self.failed_items,
            'current_phase': self.current_phase,
            'phase_progress': self.phase_progress,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'last_update': datetime.now().isoformat()
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    def load_progress(self):
        """é€²æ—ã‚’å¾©å…ƒ"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    self.total_items = progress_data.get('total_items', 0)
                    self.processed_items = progress_data.get('processed_items', 0)
                    self.failed_items = progress_data.get('failed_items', 0)
                    self.current_phase = progress_data.get('current_phase', 'æº–å‚™ä¸­')
                    self.phase_progress = progress_data.get('phase_progress', 0)
                    start_time_str = progress_data.get('start_time')
                    if start_time_str:
                        self.start_time = datetime.fromisoformat(start_time_str)
                print(f"ğŸ“Š é€²æ—ã‚’å¾©å…ƒ: {self.processed_items}/{self.total_items} å®Œäº†")
                return True
            except Exception as e:
                print(f"âš ï¸ é€²æ—å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    def fetch_page_optimized(self, url, retries=2):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒšãƒ¼ã‚¸å–å¾—"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=20)
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt == retries - 1:
                    print(f"âŒ å–å¾—å¤±æ•—: {url}")
                    return None
                time.sleep(1)
        return None
    
    def get_pagination_info(self):
        """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’æ­£ç¢ºã«å–å¾—"""
        base_url = "https://www.tcc.gr.jp/copira/"
        params = {
            'copy': '',
            'copywriter': '',
            'ad': '',
            'biz': '',
            'media': '',
            'start': '1960',
            'end': '2025',
            'target_prize': 'all'
        }
        
        url = f"{base_url}?" + "&".join([f"{k}={v}" for k, v in params.items()])
        html = self.fetch_page_optimized(url)
        
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç·ä»¶æ•°ã‚’å–å¾—
            result_text = soup.find(string=re.compile(r'\d+ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ'))
            if result_text:
                match = re.search(r'(\d+)ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ', result_text)
                if match:
                    self.total_items = int(match.group(1))
            
            # å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ§‹é€ ã‚’ç¢ºèª
            # URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª¿æŸ»
            pagination_links = soup.find_all('a', href=True)
            page_urls = []
            
            for link in pagination_links:
                href = link.get('href', '')
                if 'page/' in href or 'p=' in href:
                    page_urls.append(href)
            
            # æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®š
            if page_urls:
                page_numbers = []
                for url in page_urls:
                    # page/123/ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                    match = re.search(r'page/(\d+)', url)
                    if match:
                        page_numbers.append(int(match.group(1)))
                    # p=123 ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                    match = re.search(r'[?&]p=(\d+)', url)
                    if match:
                        page_numbers.append(int(match.group(1)))
                
                if page_numbers:
                    max_page = max(page_numbers)
                    return max_page
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä»¶æ•°ã‹ã‚‰æ¨å®š
            items_per_page = len(soup.find_all('tr')) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ã
            if items_per_page > 0:
                estimated_pages = (self.total_items + items_per_page - 1) // items_per_page
                return estimated_pages
        
        return 0
    
    def extract_all_urls_smart(self):
        """ã‚¹ãƒãƒ¼ãƒˆãªæ–¹æ³•ã§å…¨URLã‚’æŠ½å‡º"""
        print("ğŸ” å…¨ãƒ‡ãƒ¼ã‚¿ã®URLæŠ½å‡ºã‚’é–‹å§‹...")
        self.current_phase = "URLæŠ½å‡º"
        
        # æ—¢å­˜ã®URLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        if os.path.exists(self.urls_file):
            try:
                with open(self.urls_file, 'r', encoding='utf-8') as f:
                    existing_urls = json.load(f)
                if existing_urls:
                    print(f"ğŸ“ æ—¢å­˜URLãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(existing_urls)}ä»¶")
                    return existing_urls
            except:
                pass
        
        all_urls = []
        max_pages = self.get_pagination_info()
        
        if max_pages == 0:
            print("âŒ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
            return []
        
        print(f"ğŸ“Š æ¨å®šç·ä»¶æ•°: {self.total_items:,}ä»¶")
        print(f"ğŸ“„ æ¨å®šãƒšãƒ¼ã‚¸æ•°: {max_pages:,}ãƒšãƒ¼ã‚¸")
        
        base_url = "https://www.tcc.gr.jp/copira/"
        
        # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
        url_patterns = [
            lambda p: f"{base_url}page/{p}/?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all",
            lambda p: f"{base_url}?p={p}&copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all",
            lambda p: f"{base_url}?page={p}&copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all"
        ]
        
        for page_num in range(1, max_pages + 1):
            if self.stop_crawling:
                break
            
            print(f"\rğŸ“„ ãƒšãƒ¼ã‚¸ {page_num:,}/{max_pages:,} å‡¦ç†ä¸­... ", end='', flush=True)
            
            page_urls = []
            
            # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            for pattern in url_patterns:
                if page_urls:  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    break
                
                url = pattern(page_num)
                html = self.fetch_page_optimized(url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                    links = soup.find_all('a', href=True)
                    for link in links:
                        href = link.get('href', '')
                        if '/copira/id/' in href:
                            if not href.startswith('http'):
                                href = f"https://www.tcc.gr.jp{href}"
                            page_urls.append(href)
            
            if page_urls:
                all_urls.extend(page_urls)
                print(f"âœ… {len(page_urls)}ä»¶", end='')
            else:
                print(f"âŒ 0ä»¶", end='')
                # é€£ç¶šã§å¤±æ•—ã™ã‚‹å ´åˆã¯æ—©æœŸçµ‚äº†
                if page_num > 10:  # æœ€åˆã®10ãƒšãƒ¼ã‚¸ã¯ç¢ºå®Ÿã«å­˜åœ¨ã™ã‚‹ã¯ãš
                    break
            
            # é€²æ—ä¿å­˜
            if page_num % 50 == 0:
                self.save_urls_checkpoint(all_urls, page_num)
                self.phase_progress = page_num / max_pages * 100
                self.save_progress()
            
            time.sleep(self.delay * 0.5)  # URLæŠ½å‡ºã¯é«˜é€ŸåŒ–
        
        print(f"\nâœ… URLæŠ½å‡ºå®Œäº†: {len(all_urls)}ä»¶")
        
        # é‡è¤‡é™¤å»
        unique_urls = list(set(all_urls))
        print(f"ğŸ”„ é‡è¤‡é™¤å»å¾Œ: {len(unique_urls)}ä»¶")
        
        # URLã‚’ä¿å­˜
        with open(self.urls_file, 'w', encoding='utf-8') as f:
            json.dump(unique_urls, f, ensure_ascii=False, indent=2)
        
        return unique_urls
    
    def save_urls_checkpoint(self, urls, page_num):
        """URLæŠ½å‡ºã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ"""
        checkpoint_data = {
            'urls': urls,
            'last_page': page_num,
            'timestamp': datetime.now().isoformat()
        }
        checkpoint_file = os.path.join(self.output_dir, f"url_checkpoint_p{page_num}.json")
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    def parse_detail_page_optimized(self, html_content, url):
        """æœ€é©åŒ–ã•ã‚ŒãŸè©³ç´°ãƒšãƒ¼ã‚¸è§£æ"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            data = {
                'url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            # IDã‚’æŠ½å‡º
            id_match = re.search(r'/id/(\d+)', url)
            if id_match:
                data['id'] = id_match.group(1)
            
            # ãƒ¡ã‚¤ãƒ³æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
            info_table = soup.find('table', class_='table1__table')
            if info_table:
                rows = info_table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                        value = td.get_text(strip=True)
                        
                        # æ­£è¦åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°
                        key_mapping = {
                            'åºƒå‘Šä¸»': 'advertiser',
                            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ': 'advertiser',
                            'å—è³': 'award',
                            'æ¥­ç¨®': 'industry',
                            'åª’ä½“': 'media_type',
                            'æ²è¼‰å¹´åº¦': 'publication_year',
                            'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                            'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                            'åºƒå‘Šä¼šç¤¾': 'agency',
                            'åˆ¶ä½œä¼šç¤¾': 'production_company'
                        }
                        
                        normalized_key = key_mapping.get(key, key.lower().replace(' ', '_'))
                        data[normalized_key] = value
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                data['title'] = title_elem.get_text(strip=True)
            
            # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ãƒªãƒ³ã‚¯
            copywriter_links = soup.find_all('a', href=lambda x: x and '/copitan/' in x)
            if copywriter_links:
                data['copywriter_links'] = []
                for link in copywriter_links:
                    cw_data = {
                        'name': link.get_text(strip=True),
                        'url': link.get('href', '')
                    }
                    id_match = re.search(r'/id/(\d+)', link.get('href', ''))
                    if id_match:
                        cw_data['id'] = id_match.group(1)
                    data['copywriter_links'].append(cw_data)
            
            return data
            
        except Exception as e:
            return {'error': str(e), 'url': url}
    
    def crawl_details_batch(self, urls):
        """è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«ãƒãƒƒãƒå–å¾—"""
        print(f"\nğŸ“¥ è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {len(urls):,}ä»¶")
        self.current_phase = "è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—"
        
        all_data = []
        
        for i, url in enumerate(urls, 1):
            if self.stop_crawling:
                break
            
            if i % 100 == 0 or i == 1:
                elapsed = datetime.now() - self.start_time if self.start_time else timedelta(0)
                rate = i / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
                eta_seconds = (len(urls) - i) / rate if rate > 0 else 0
                eta = str(timedelta(seconds=int(eta_seconds)))
                
                print(f"ğŸ“Š {i:,}/{len(urls):,} ({i/len(urls)*100:.1f}%) "
                      f"é€Ÿåº¦:{rate:.1f}ä»¶/ç§’ ETA:{eta}")
            
            html = self.fetch_page_optimized(url)
            if html:
                data = self.parse_detail_page_optimized(html, url)
                all_data.append(data)
                self.processed_items += 1
            else:
                self.failed_items += 1
            
            # ãƒãƒƒãƒä¿å­˜
            if i % self.batch_size == 0:
                self.save_batch_data(all_data, i)
                self.phase_progress = i / len(urls) * 100
                self.save_progress()
            
            time.sleep(self.delay)
        
        return all_data
    
    def save_batch_data(self, data, batch_num):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        batch_file = os.path.join(self.output_dir, f"batch_data_{batch_num:06d}.json")
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ãƒãƒƒãƒä¿å­˜: {batch_file}")
    
    def consolidate_all_data(self):
        """å…¨ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        print("\nğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        self.current_phase = "ãƒ‡ãƒ¼ã‚¿çµ±åˆ"
        
        all_data = []
        batch_files = [f for f in os.listdir(self.output_dir) if f.startswith('batch_data_')]
        batch_files.sort()
        
        for batch_file in batch_files:
            file_path = os.path.join(self.output_dir, batch_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                    all_data.extend(batch_data)
                print(f"ğŸ“ çµ±åˆ: {batch_file} ({len(batch_data)}ä»¶)")
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {batch_file} - {e}")
        
        return all_data
    
    def save_final_complete_data(self, data):
        """æœ€çµ‚å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Œå…¨JSONãƒ•ã‚¡ã‚¤ãƒ«
        json_file = os.path.join(self.output_dir, f"tcc_complete_data_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # çµ±è¨ˆCSVãƒ•ã‚¡ã‚¤ãƒ«
        csv_file = os.path.join(self.output_dir, f"tcc_complete_summary_{timestamp}.csv")
        self.create_summary_csv(data, csv_file)
        
        # å®Œå…¨åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
        analysis_file = os.path.join(self.output_dir, f"tcc_complete_analysis_{timestamp}.txt")
        self.create_complete_analysis(data, analysis_file)
        
        print(f"\nğŸ’¾ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        print(f"  ğŸ“‹ å®Œå…¨JSON: {json_file}")
        print(f"  ğŸ“Š çµ±è¨ˆCSV: {csv_file}")
        print(f"  ğŸ“ˆ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {analysis_file}")
        
        return json_file, csv_file, analysis_file
    
    def create_summary_csv(self, data, csv_file):
        """çµ±è¨ˆã‚µãƒãƒªãƒ¼CSVã‚’ä½œæˆ"""
        import csv
        
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = [
                'id', 'title', 'advertiser', 'award', 'industry', 'media_type',
                'publication_year', 'page_number', 'copywriter', 'agency',
                'production_company', 'copywriter_count', 'has_award',
                'url', 'has_error'
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for item in data:
                copywriter_count = len(item.get('copywriter_links', []))
                
                row = {
                    'id': item.get('id', ''),
                    'title': item.get('title', ''),
                    'advertiser': item.get('advertiser', ''),
                    'award': item.get('award', ''),
                    'industry': item.get('industry', ''),
                    'media_type': item.get('media_type', ''),
                    'publication_year': item.get('publication_year', ''),
                    'page_number': item.get('page_number', ''),
                    'copywriter': item.get('copywriter', ''),
                    'agency': item.get('agency', ''),
                    'production_company': item.get('production_company', ''),
                    'copywriter_count': copywriter_count,
                    'has_award': 'Yes' if item.get('award') else 'No',
                    'url': item.get('url', ''),
                    'has_error': 'Yes' if 'error' in item else 'No'
                }
                writer.writerow(row)
    
    def create_complete_analysis(self, data, analysis_file):
        """å®Œå…¨åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        valid_data = [item for item in data if 'error' not in item]
        
        lines = []
        lines.append("=" * 80)
        lines.append("TCC ã‚³ãƒ”ãƒ© å®Œå…¨ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        lines.append("=" * 80)
        lines.append(f"ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        lines.append(f"ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data):,}ä»¶")
        lines.append(f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(valid_data):,}ä»¶")
        lines.append(f"ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {len(data) - len(valid_data):,}ä»¶")
        lines.append("")
        
        # å¹´åº¦åˆ¥åˆ†æ
        year_stats = {}
        for item in valid_data:
            year = item.get('publication_year', 'Unknown')
            year_stats[year] = year_stats.get(year, 0) + 1
        
        lines.append("ğŸ“… å¹´åº¦åˆ¥çµ±è¨ˆ (ä¸Šä½20å¹´):")
        lines.append("-" * 40)
        for year, count in sorted(year_stats.items(), key=lambda x: x[1], reverse=True)[:20]:
            if year != 'Unknown':
                lines.append(f"{year}å¹´: {count:,}ä»¶")
        lines.append("")
        
        # åª’ä½“åˆ¥åˆ†æ
        media_stats = {}
        for item in valid_data:
            media = item.get('media_type', 'Unknown')
            media_stats[media] = media_stats.get(media, 0) + 1
        
        lines.append("ğŸ“º åª’ä½“åˆ¥çµ±è¨ˆ:")
        lines.append("-" * 40)
        for media, count in sorted(media_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(valid_data)) * 100
            lines.append(f"{media:15s}: {count:,}ä»¶ ({percentage:5.1f}%)")
        lines.append("")
        
        # æ¥­ç¨®åˆ¥åˆ†æ
        industry_stats = {}
        for item in valid_data:
            industry = item.get('industry', 'Unknown')
            industry_stats[industry] = industry_stats.get(industry, 0) + 1
        
        lines.append("ğŸ¢ æ¥­ç¨®åˆ¥çµ±è¨ˆ (ä¸Šä½20æ¥­ç¨®):")
        lines.append("-" * 40)
        for industry, count in sorted(industry_stats.items(), key=lambda x: x[1], reverse=True)[:20]:
            if industry != 'Unknown':
                lines.append(f"{industry:30s}: {count:,}ä»¶")
        lines.append("")
        
        # å—è³çµ±è¨ˆ
        award_stats = {}
        for item in valid_data:
            award = item.get('award', 'None')
            if award and award != 'None':
                award_stats[award] = award_stats.get(award, 0) + 1
        
        lines.append("ğŸ† å—è³çµ±è¨ˆ:")
        lines.append("-" * 40)
        for award, count in sorted(award_stats.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"{award:30s}: {count:,}ä»¶")
        
        lines.append("\n" + "=" * 80)
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
    
    def run_complete_crawl(self):
        """å®Œå…¨ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ TCC ã‚³ãƒ”ãƒ© å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹")
        print("=" * 60)
        
        # é€²æ—å¾©å…ƒã‚’è©¦è¡Œ
        resumed = self.load_progress()
        if not resumed:
            self.start_time = datetime.now()
        
        try:
            # ãƒ•ã‚§ãƒ¼ã‚º1: URLæŠ½å‡º
            if not os.path.exists(self.urls_file):
                urls = self.extract_all_urls_smart()
            else:
                with open(self.urls_file, 'r', encoding='utf-8') as f:
                    urls = json.load(f)
                print(f"ğŸ“ æ—¢å­˜URLãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {len(urls)}ä»¶")
            
            if not urls:
                print("âŒ URLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—
            all_data = self.crawl_details_batch(urls)
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            if len(all_data) == 0:
                all_data = self.consolidate_all_data()
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: æœ€çµ‚ä¿å­˜
            if all_data:
                self.save_final_complete_data(all_data)
                self.print_final_summary(all_data)
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            self.save_progress()
    
    def print_final_summary(self, data):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        elapsed = datetime.now() - self.start_time if self.start_time else timedelta(0)
        valid_data = [item for item in data if 'error' not in item]
        
        print(f"\nğŸ‰ å®Œå…¨ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†!")
        print("=" * 60)
        print(f"ğŸ“Š ç·å–å¾—ä»¶æ•°: {len(data):,}")
        print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_data):,}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {len(data) - len(valid_data):,}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {len(valid_data)/len(data)*100:.1f}%")
        print(f"â±ï¸ ç·æ‰€è¦æ™‚é–“: {elapsed}")
        if elapsed.total_seconds() > 0:
            print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(data)/elapsed.total_seconds():.2f}ä»¶/ç§’")
        print("=" * 60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("TCC ã‚³ãƒ”ãƒ© å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ v2.0")
    print("=" * 50)
    print("âš ï¸ 37,259ä»¶ã®å®Œå…¨å–å¾—ã‚’é–‹å§‹ã—ã¾ã™")
    print("â±ï¸ æ¨å®šæ‰€è¦æ™‚é–“: 5-8æ™‚é–“")
    print("ğŸ›‘ Ctrl+C ã§å®‰å…¨ã«ä¸­æ–­ã§ãã¾ã™")
    print("")
    
    crawler = OptimizedTCCFullCrawler(
        output_dir="tcc_complete_data",
        delay=0.5,  # 0.5ç§’é–“éš”
        batch_size=100
    )
    
    crawler.run_complete_crawl()

if __name__ == "__main__":
    main()