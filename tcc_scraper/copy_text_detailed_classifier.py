#!/usr/bin/env python3
"""
TCC ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆè©³ç´°åˆ†é¡å™¨ - æœ€çµ‚ç‰ˆ
ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’7ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«è©³ç´°åˆ†é¡
"""
import gzip
import json
import os
import re
from bs4 import BeautifulSoup, NavigableString, Tag
from collections import defaultdict
from datetime import datetime
import csv

class CopyTextDetailedClassifier:
    def __init__(self):
        self.html_dir = "complete_html_data"
        
    def extract_and_classify_copy(self, html_content, tcc_id):
        """HTMLã‹ã‚‰ã‚³ãƒ”ãƒ¼è¦ç´ ã‚’æŠ½å‡ºãƒ»è©³ç´°åˆ†é¡"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        result = {
            'tcc_id': int(tcc_id),
            'main_headline': None,      # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ£ãƒƒãƒãƒ•ãƒ¬ãƒ¼ã‚º
            'sub_headline': None,       # ã‚µãƒ–ã‚­ãƒ£ãƒƒãƒãƒ•ãƒ¬ãƒ¼ã‚º
            'body_copy': None,          # ãƒœãƒ‡ã‚£ã‚³ãƒ”ãƒ¼
            'dialogue': None,           # ä¼šè©±ãƒ»ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            'tagline': None,           # ã‚¿ã‚°ãƒ©ã‚¤ãƒ³ãƒ»ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°
            'product_info': None,      # å•†å“ãƒ»ä¼æ¥­æƒ…å ±
            'notes': None,             # æ³¨é‡ˆãƒ»å‚™è€ƒ
            'raw_copy_text': None,     # å…¨ä½“ã®ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
        }
        
        # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼è¦ç´ ã‚’åˆ†æ
        catch_elem = soup.find('p', class_='block5-1__catch')
        if catch_elem:
            copy_analysis = self.analyze_catch_structure(catch_elem)
            result.update(copy_analysis)
        
        # ãƒãƒ¼ãƒˆè¦ç´ 
        notes_elem = soup.find('p', class_='block5-1__notes')
        if notes_elem:
            notes_text = notes_elem.get_text(strip=True)
            if notes_text:
                result['notes'] = notes_text
        
        # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå•†å“æƒ…å ±ã¨ã—ã¦ï¼‰
        title_elem = soup.find('h1')
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if title_text:
                result['product_info'] = title_text
        
        return result
    
    def analyze_catch_structure(self, catch_elem):
        """ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼è¦ç´ ã®è©³ç´°æ§‹é€ åˆ†æ"""
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        full_text = catch_elem.get_text(strip=True)
        
        # ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã¨spanå†…ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é›¢
        direct_texts = []
        span_texts = []
        
        for content in catch_elem.contents:
            if isinstance(content, NavigableString):
                text = str(content).strip()
                if text:
                    direct_texts.append(text)
            elif isinstance(content, Tag):
                if content.name == 'span':
                    # spanå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                    span_content = self.extract_span_content(content)
                    span_texts.extend(span_content)
                elif content.name == 'br':
                    # æ”¹è¡Œã¯ç„¡è¦–
                    continue
        
        # ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        main_text = ' '.join(direct_texts) if direct_texts else None
        
        # spanå†…ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        dialogue_text = '\n'.join(span_texts) if span_texts else None
        
        # åˆ†é¡çµæœ
        result = {
            'raw_copy_text': full_text,
        }
        
        # åˆ†é¡ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«é…ç½®
        if main_text:
            classification = self.classify_text_type(main_text)
            if classification == 'main_headline':
                result['main_headline'] = main_text
            elif classification == 'sub_headline':
                result['sub_headline'] = main_text
            elif classification == 'tagline':
                result['tagline'] = main_text
            elif classification == 'product_info':
                if not result.get('product_info'):  # æ—¢å­˜ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¸Šæ›¸ãã—ãªã„
                    result['product_info'] = main_text
            else:
                result['body_copy'] = main_text
        
        if dialogue_text:
            classification = self.classify_text_type(dialogue_text)
            if classification == 'dialogue':
                result['dialogue'] = dialogue_text
            elif classification == 'body_copy':
                result['body_copy'] = dialogue_text
            else:
                # dialogueã¨ã—ã¦ä¿å­˜
                result['dialogue'] = dialogue_text
        
        return result
    
    def extract_span_content(self, span_elem):
        """spanè¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆè¡Œã‚’æŠ½å‡º"""
        lines = []
        current_line = ""
        
        for content in span_elem.contents:
            if isinstance(content, NavigableString):
                current_line += str(content)
            elif isinstance(content, Tag) and content.name == 'br':
                if current_line.strip():
                    lines.append(current_line.strip())
                current_line = ""
        
        # æœ€å¾Œã®è¡Œã‚’è¿½åŠ 
        if current_line.strip():
            lines.append(current_line.strip())
        
        return lines
    
    def classify_text_type(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        if not text:
            return 'unknown'
        
        text_clean = text.strip()
        
        # ä¼šè©±ãƒ»ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¯¾è©±å½¢å¼ï¼‰
        if re.search(r'[ï¼š:]\s*[ã€Œã€"]|ã„ã¨ã—|ã“ã„ã—|ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³|N:|S:|M:|NA|å¡™|æ¡ƒå¤ªéƒ|æµ¦å³¶å¤ªéƒ|ä¸€å¯¸æ³•å¸«|é‡‘å¤ªéƒ|ã‹ãã‚„å§«', text_clean):
            return 'dialogue'
        
        # å•†å“åãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ï¼ˆä¼šç¤¾åã€å•†å“åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        if re.search(r'æ ªå¼ä¼šç¤¾|ä¼šç¤¾|è£½è–¬|ç”£æ¥­|é›»å™¨|è‡ªå‹•è»Š|ãƒ“ãƒ¼ãƒ«|é£Ÿå“|ã‚³ã‚«ãƒ»ã‚³ãƒ¼ãƒ©|é™¤è™«èŠ|ãƒã‚¯ãƒ‰ãƒŠãƒ«ãƒ‰', text_clean):
            return 'product_info'
        
        # ã‚¿ã‚°ãƒ©ã‚¤ãƒ³ï¼ˆçŸ­ãã¦ã‚­ãƒ£ãƒƒãƒãƒ¼ã€ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
        if len(text_clean) < 50 and (
            re.search(r'ã«ã¯|ã ã‘|ã§ã™|ã§ã‚ã‚‹|ã $|ã—ã‚ˆã†|ã—ã¾ã—ã‚‡ã†|å¥½ãã§ã™|ã‚ã‚Šã¾ã™|ã—ã¦ã‚‹é–“ã¯', text_clean) or
            not re.search(r'[ã€‚ã€]', text_clean)
        ):
            return 'tagline'
        
        # ãƒ¡ã‚¤ãƒ³ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆä¸­ç¨‹åº¦ã®é•·ã•ã€å•†å“åå«ã‚€ï¼‰
        if 10 <= len(text_clean) <= 100:
            return 'main_headline'
        
        # ã‚µãƒ–ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆã‚„ã‚„é•·ã‚ï¼‰
        if 100 < len(text_clean) <= 200:
            return 'sub_headline'
        
        # ãƒœãƒ‡ã‚£ã‚³ãƒ”ãƒ¼ï¼ˆé•·æ–‡ï¼‰
        if len(text_clean) > 200:
            return 'body_copy'
        
        # çŸ­ã™ãã‚‹å ´åˆã¯ãã®ä»–
        return 'main_headline'
    
    def process_sample_files(self, num_samples=100):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
        html_files = [f for f in os.listdir(self.html_dir) if f.endswith('.html.gz')][:num_samples]
        
        results = []
        classification_stats = defaultdict(int)
        
        print(f"ğŸ” {len(html_files)}ä»¶ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æä¸­...")
        
        for i, html_file in enumerate(html_files):
            tcc_id = html_file.replace('tcc_', '').replace('.html.gz', '')
            file_path = os.path.join(self.html_dir, html_file)
            
            try:
                with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                    html_content = f.read()
                
                result = self.extract_and_classify_copy(html_content, tcc_id)
                results.append(result)
                
                # çµ±è¨ˆæ›´æ–°
                for field in ['main_headline', 'sub_headline', 'body_copy', 'dialogue', 'tagline', 'product_info', 'notes']:
                    if result.get(field):
                        classification_stats[field] += 1
                
                if (i + 1) % 20 == 0:
                    print(f"   é€²æ—: {i+1}/{len(html_files)}")
                
            except Exception as e:
                print(f"âŒ ID {tcc_id}: ã‚¨ãƒ©ãƒ¼ - {e}")
        
        return results, classification_stats
    
    def save_enhanced_dataset(self, results, stats):
        """æ”¹è‰¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ†é¡æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        dataset_file = f"tcc_classified_copy_dataset_{timestamp}.json"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # CSVå½¢å¼ã‚‚ç”Ÿæˆ
        csv_file = f"tcc_classified_copy_dataset_{timestamp}.csv"
        
        if results:
            fieldnames = ['tcc_id', 'main_headline', 'sub_headline', 'body_copy', 'dialogue', 
                         'tagline', 'product_info', 'notes', 'raw_copy_text']
            
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for result in results:
                    row = {}
                    for field in fieldnames:
                        value = result.get(field, '')
                        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ¶é™
                        if isinstance(value, str) and len(value) > 5000:
                            value = value[:5000] + '...'
                        row[field] = value
                    writer.writerow(row)
        
        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
        print(f"   ğŸ“Š JSON: {dataset_file}")
        print(f"   ğŸ“‹ CSV: {csv_file}")
        
        return dataset_file, csv_file
    
    def run_classification(self):
        """åˆ†é¡å‡¦ç†ã‚’å®Ÿè¡Œ"""
        print("ğŸ¯ TCC ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆè©³ç´°åˆ†é¡é–‹å§‹")
        print("=" * 60)
        print("åˆ†é¡ã‚«ãƒ†ã‚´ãƒª:")
        print("  ğŸ“¢ main_headline    : ãƒ¡ã‚¤ãƒ³ã‚­ãƒ£ãƒƒãƒãƒ•ãƒ¬ãƒ¼ã‚º")
        print("  ğŸ“ sub_headline     : ã‚µãƒ–ã‚­ãƒ£ãƒƒãƒãƒ•ãƒ¬ãƒ¼ã‚º") 
        print("  ğŸ“– body_copy        : ãƒœãƒ‡ã‚£ã‚³ãƒ”ãƒ¼")
        print("  ğŸ’¬ dialogue         : ä¼šè©±ãƒ»ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        print("  ğŸ·ï¸  tagline          : ã‚¿ã‚°ãƒ©ã‚¤ãƒ³ãƒ»ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°")
        print("  ğŸ“¦ product_info     : å•†å“ãƒ»ä¼æ¥­æƒ…å ±")
        print("  ğŸ“„ notes            : æ³¨é‡ˆãƒ»å‚™è€ƒ")
        print("=" * 60)
        
        # ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†
        results, stats = self.process_sample_files(200)
        
        # çµæœä¿å­˜
        files = self.save_enhanced_dataset(results, stats)
        
        # çµ±è¨ˆè¡¨ç¤º
        print(f"\nğŸ“Š åˆ†é¡çµæœ:")
        total = len(results)
        for classification, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total * 100) if total > 0 else 0
            print(f"   {classification:15s}: {count:4d}ä»¶ ({percentage:5.1f}%)")
        
        print(f"\nâœ… å®Œäº†: {len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°åˆ†é¡ã—ã¾ã—ãŸ")
        
        return results, stats

if __name__ == "__main__":
    classifier = CopyTextDetailedClassifier()
    classifier.run_classification()