import requests
from bs4 import BeautifulSoup
import json
import time
import re

def fetch_page_content(page_num=1):
    """ç‰¹å®šã®ãƒšãƒ¼ã‚¸ã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    base_url = "https://www.tcc.gr.jp/copira/"
    
    # ãƒšãƒ¼ã‚¸ç•ªå·ä»˜ãã®URLã‚’æ§‹ç¯‰
    if page_num == 1:
        url = f"{base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all"
    else:
        url = f"{base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&page={page_num}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching page {page_num}: {e}")
        return None

def parse_entries(html_content):
    """HTMLã‹ã‚‰åºƒå‘Šã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html_content, 'html.parser')
    entries = []
    
    # æ¤œç´¢çµæœã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
    table = soup.find('table', {'class': 'search-result'}) or soup.find('table')
    
    if table:
        # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        rows = table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 4:
                entry = {
                    'title': cols[0].get_text(strip=True),
                    'advertiser': cols[1].get_text(strip=True),
                    'copywriter': cols[2].get_text(strip=True),
                    'media': cols[3].get_text(strip=True) if len(cols) > 3 else '',
                }
                
                # è©³ç´°ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°å–å¾—
                link = cols[0].find('a')
                if link:
                    entry['detail_url'] = link.get('href', '')
                
                entries.append(entry)
    else:
        # ãƒªã‚¹ãƒˆå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            ('div', {'class': 'result-item'}),
            ('div', {'class': 'entry'}),
            ('article', {}),
            ('li', {'class': 'item'}),
            ('div', {'class': 'work-item'}),
        ]
        
        for tag, attrs in patterns:
            items = soup.find_all(tag, attrs)
            if items:
                for item in items:
                    entry = extract_entry_data(item)
                    if entry and any(entry.values()):  # å°‘ãªãã¨ã‚‚1ã¤ã®å€¤ãŒã‚ã‚Œã°è¿½åŠ 
                        entries.append(entry)
                break
    
    # ã‚‚ã—ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã‚ˆã‚Šæ±ç”¨çš„ãªæ–¹æ³•ã‚’è©¦ã™
    if not entries:
        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‹ã‚‰åºƒå‘Šãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        links = soup.find_all('a', href=True)
        for link in links:
            if 'detail' in link.get('href', '') or 'id=' in link.get('href', ''):
                parent = link.parent
                if parent:
                    entry = extract_entry_data(parent)
                    if entry and entry.get('title'):
                        entries.append(entry)
    
    return entries

def extract_entry_data(element):
    """HTMLè¦ç´ ã‹ã‚‰åºƒå‘Šãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    entry = {}
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
    title_elem = element.find(['h2', 'h3', 'h4', 'a'])
    if title_elem:
        entry['title'] = title_elem.get_text(strip=True)
    
    # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
    text = element.get_text(separator=' ', strip=True)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§æƒ…å ±ã‚’æŠ½å‡º
    patterns = {
        'advertiser': r'åºƒå‘Šä¸»[ï¼š:]\s*([^\s]+)',
        'copywriter': r'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼[ï¼š:]\s*([^\s]+)',
        'media': r'åª’ä½“[ï¼š:]\s*([^\s]+)',
        'award': r'å—è³[ï¼š:]\s*([^\s]+)',
        'year': r'(\d{4})å¹´',
        'industry': r'æ¥­ç¨®[ï¼š:]\s*([^\s]+)',
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, text)
        if match:
            entry[key] = match.group(1)
    
    # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®š
    media_types = ['TVCM', 'ãƒ©ã‚¸ã‚ªCM', 'ãƒã‚¹ã‚¿ãƒ¼', 'WEB', 'æ–°è', 'é›‘èªŒ', 'OOH']
    for media in media_types:
        if media in text:
            entry['media'] = media
            break
    
    return entry

def fetch_detail_page(detail_url):
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—"""
    if not detail_url or not detail_url.startswith('http'):
        if detail_url and detail_url.startswith('/'):
            detail_url = f"https://www.tcc.gr.jp{detail_url}"
        else:
            return {}
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(detail_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        details = {}
        
        # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        text = soup.get_text(separator=' ', strip=True)
        
        # è¿½åŠ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        patterns = {
            'campaign': r'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³[ï¼š:]\s*([^ã€ã€‚\n]+)',
            'agency': r'åºƒå‘Šä¼šç¤¾[ï¼š:]\s*([^ã€ã€‚\n]+)',
            'production': r'åˆ¶ä½œä¼šç¤¾[ï¼š:]\s*([^ã€ã€‚\n]+)',
            'director': r'ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼[ï¼š:]\s*([^ã€ã€‚\n]+)',
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, text)
            if match:
                details[key] = match.group(1)
        
        return details
    except Exception as e:
        print(f"Error fetching detail page: {e}")
        return {}

def scrape_tcc_copira(max_items=100, fetch_details=False):
    """TCCã‚³ãƒ”ãƒ©ã‹ã‚‰åºƒå‘Šãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    all_entries = []
    page = 1
    items_per_page = 20  # æ¨å®šå€¤
    
    print(f"Starting to scrape {max_items} items from TCC Copira...")
    
    while len(all_entries) < max_items:
        print(f"\nFetching page {page}...")
        html_content = fetch_page_content(page)
        
        if not html_content:
            print(f"Failed to fetch page {page}")
            break
        
        entries = parse_entries(html_content)
        
        if not entries:
            print(f"No entries found on page {page}")
            # æ¬¡ã®ãƒšãƒ¼ã‚¸ã‚’è©¦ã™
            if page < 10:  # æœ€åˆã®10ãƒšãƒ¼ã‚¸ã¯è©¦ã™
                page += 1
                continue
            else:
                break
        
        print(f"Found {len(entries)} entries on page {page}")
        
        for entry in entries:
            if len(all_entries) >= max_items:
                break
            
            # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if fetch_details and entry.get('detail_url'):
                details = fetch_detail_page(entry['detail_url'])
                entry.update(details)
                time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
            
            all_entries.append(entry)
            print(f"  [{len(all_entries)}] {entry.get('title', 'No title')} - {entry.get('media', 'Unknown media')}")
        
        page += 1
        time.sleep(1)  # ãƒšãƒ¼ã‚¸é–“ã§å¾…æ©Ÿ
    
    return all_entries

def analyze_data(entries):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ"""
    analysis = {
        'total_count': len(entries),
        'by_media': {},
        'by_year': {},
        'by_advertiser': {},
        'by_copywriter': {},
        'has_title': 0,
        'has_advertiser': 0,
        'has_copywriter': 0,
        'has_media': 0,
    }
    
    for entry in entries:
        # ãƒ¡ãƒ‡ã‚£ã‚¢åˆ¥é›†è¨ˆ
        media = entry.get('media', 'Unknown')
        analysis['by_media'][media] = analysis['by_media'].get(media, 0) + 1
        
        # å¹´åº¦åˆ¥é›†è¨ˆ
        year = entry.get('year', 'Unknown')
        analysis['by_year'][year] = analysis['by_year'].get(year, 0) + 1
        
        # åºƒå‘Šä¸»åˆ¥é›†è¨ˆ
        advertiser = entry.get('advertiser', 'Unknown')
        analysis['by_advertiser'][advertiser] = analysis['by_advertiser'].get(advertiser, 0) + 1
        
        # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼åˆ¥é›†è¨ˆ
        copywriter = entry.get('copywriter', 'Unknown')
        analysis['by_copywriter'][copywriter] = analysis['by_copywriter'].get(copywriter, 0) + 1
        
        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        if entry.get('title'):
            analysis['has_title'] += 1
        if entry.get('advertiser'):
            analysis['has_advertiser'] += 1
        if entry.get('copywriter'):
            analysis['has_copywriter'] += 1
        if entry.get('media'):
            analysis['has_media'] += 1
    
    return analysis

def main():
    # 100ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    entries = scrape_tcc_copira(max_items=100, fetch_details=False)
    
    if entries:
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        with open('tcc_data_100.json', 'w', encoding='utf-8') as f:
            json.dump(entries, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… Saved {len(entries)} entries to tcc_data_100.json")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†æ
        analysis = analyze_data(entries)
        
        # åˆ†æçµæœã‚’è¡¨ç¤º
        print("\n" + "="*50)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ")
        print("="*50)
        print(f"ç·ä»¶æ•°: {analysis['total_count']}")
        
        print(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§:")
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«ã‚ã‚Š: {analysis['has_title']}/{analysis['total_count']} ({analysis['has_title']/analysis['total_count']*100:.1f}%)")
        print(f"  åºƒå‘Šä¸»ã‚ã‚Š: {analysis['has_advertiser']}/{analysis['total_count']} ({analysis['has_advertiser']/analysis['total_count']*100:.1f}%)")
        print(f"  ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã‚ã‚Š: {analysis['has_copywriter']}/{analysis['total_count']} ({analysis['has_copywriter']/analysis['total_count']*100:.1f}%)")
        print(f"  åª’ä½“ã‚ã‚Š: {analysis['has_media']}/{analysis['total_count']} ({analysis['has_media']/analysis['total_count']*100:.1f}%)")
        
        print(f"\nğŸ“º åª’ä½“åˆ¥é›†è¨ˆ (ä¸Šä½10):")
        for media, count in sorted(analysis['by_media'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {media}: {count}ä»¶")
        
        print(f"\nğŸ“… å¹´åº¦åˆ¥é›†è¨ˆ (ä¸Šä½10):")
        for year, count in sorted(analysis['by_year'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {year}: {count}ä»¶")
        
        print(f"\nğŸ¢ åºƒå‘Šä¸»åˆ¥é›†è¨ˆ (ä¸Šä½10):")
        for advertiser, count in sorted(analysis['by_advertiser'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {advertiser}: {count}ä»¶")
        
        print(f"\nâœï¸ ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼åˆ¥é›†è¨ˆ (ä¸Šä½10):")
        for copywriter, count in sorted(analysis['by_copywriter'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {copywriter}: {count}ä»¶")
        
        # åˆ†æçµæœã‚‚ä¿å­˜
        with open('tcc_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… åˆ†æçµæœã‚’ tcc_analysis.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()