#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os

class DirectTCCCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://www.tcc.gr.jp/copira/"
        self.all_data = []
        self.processed = 0
        self.failed = 0
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs("tcc_final_data", exist_ok=True)
    
    def log(self, message):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        print(f"[{timestamp}] {message}")
        
    def get_page(self, url, retries=3):
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt == retries - 1:
                    self.log(f"âŒ Failed: {url}")
                    return None
                time.sleep(2)
        return None
    
    def find_all_detail_urls(self):
        """å…¨ã¦ã®è©³ç´°URLã‚’ç™ºè¦‹"""
        self.log("ğŸ” Starting comprehensive URL discovery...")
        
        all_urls = set()
        
        # è¤‡æ•°ã®æˆ¦ç•¥ã‚’è©¦è¡Œ
        strategies = [
            self.strategy_pagination_links,
            self.strategy_direct_enumeration,
            self.strategy_search_by_year
        ]
        
        for strategy in strategies:
            self.log(f"ğŸ”„ Trying strategy: {strategy.__name__}")
            urls = strategy()
            all_urls.update(urls)
            self.log(f"âœ… Found {len(urls)} URLs, Total: {len(all_urls)}")
            
            if len(all_urls) > 35000:  # ååˆ†ãªæ•°ãŒè¦‹ã¤ã‹ã£ãŸã‚‰åœæ­¢
                break
        
        self.log(f"ğŸ¯ Total URLs discovered: {len(all_urls)}")
        return list(all_urls)
    
    def strategy_pagination_links(self):
        """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’è¾¿ã‚‹æˆ¦ç•¥"""
        urls = set()
        
        # ç•°ãªã‚‹è¡¨ç¤ºä»¶æ•°ã§è©¦è¡Œ
        for limit in [10, 20, 50]:
            page = 1
            consecutive_failures = 0
            
            while consecutive_failures < 5:
                url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit={limit}&page={page}"
                
                html = self.get_page(url)
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # è©³ç´°ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                    links = soup.find_all('a', href=True)
                    page_urls = []
                    for link in links:
                        href = link.get('href', '')
                        if '/copira/id/' in href:
                            if not href.startswith('http'):
                                href = f"https://www.tcc.gr.jp{href}"
                            page_urls.append(href)
                    
                    if page_urls:
                        urls.update(page_urls)
                        consecutive_failures = 0
                        if page % 100 == 0:
                            self.log(f"  Page {page}: {len(page_urls)} URLs")
                    else:
                        consecutive_failures += 1
                else:
                    consecutive_failures += 1
                
                page += 1
                time.sleep(0.5)
                
                # å®‰å…¨ãªä¸Šé™
                if page > 2000:
                    break
        
        return urls
    
    def strategy_direct_enumeration(self):
        """ç›´æ¥IDåˆ—æŒ™æˆ¦ç•¥"""
        urls = set()
        
        # æ—¢çŸ¥ã®IDãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¦ç¯„å›²ã‚’æ¨å®š
        sample_ids = [2023001, 2023352, 2024001]  # ã‚µãƒ³ãƒ—ãƒ«
        
        # å¹´åº¦åˆ¥ã«IDç¯„å›²ã‚’æ¨å®š
        for year in range(1960, 2026):
            start_id = year * 1000
            end_id = start_id + 999
            
            self.log(f"  Checking year {year}: {start_id}-{end_id}")
            
            found_in_year = 0
            for id_num in range(start_id, end_id + 1):
                url = f"https://www.tcc.gr.jp/copira/id/{id_num}/"
                
                # è»½é‡ãƒã‚§ãƒƒã‚¯ï¼ˆHEADãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
                try:
                    response = self.session.head(url, timeout=10)
                    if response.status_code == 200:
                        urls.add(url)
                        found_in_year += 1
                except:
                    pass
                
                if id_num % 50 == 0:
                    time.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            if found_in_year > 0:
                self.log(f"    Found {found_in_year} URLs for {year}")
        
        return urls
    
    def strategy_search_by_year(self):
        """å¹´åº¦åˆ¥æ¤œç´¢æˆ¦ç•¥"""
        urls = set()
        
        for year in range(1960, 2026):
            url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start={year}&end={year}&target_prize=all"
            
            html = self.get_page(url)
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                
                # ã“ã®å¹´åº¦ã®å…¨ãƒšãƒ¼ã‚¸ã‚’è¾¿ã‚‹
                page = 1
                while page < 1000:  # å®‰å…¨ä¸Šé™
                    page_url = f"{url}&page={page}"
                    page_html = self.get_page(page_url)
                    
                    if page_html:
                        page_soup = BeautifulSoup(page_html, 'html.parser')
                        links = page_soup.find_all('a', href=True)
                        
                        page_urls = []
                        for link in links:
                            href = link.get('href', '')
                            if '/copira/id/' in href:
                                if not href.startswith('http'):
                                    href = f"https://www.tcc.gr.jp{href}"
                                page_urls.append(href)
                        
                        if page_urls:
                            urls.update(page_urls)
                        else:
                            break  # ã“ã®ãƒšãƒ¼ã‚¸ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°çµ‚äº†
                    else:
                        break
                    
                    page += 1
                    time.sleep(0.3)
            
            self.log(f"  Year {year}: {len(urls)} total URLs so far")
        
        return urls
    
    def extract_detail_data(self, url):
        """è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        html = self.get_page(url)
        if not html:
            return {'error': 'fetch_failed', 'url': url}
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data = {'url': url, 'scraped_at': datetime.now().isoformat()}
            
            # IDæŠ½å‡º
            id_match = re.search(r'/id/(\d+)', url)
            if id_match:
                data['id'] = id_match.group(1)
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            title = soup.find('h1') or soup.find('title')
            if title:
                data['title'] = title.get_text(strip=True)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                        value = td.get_text(strip=True)
                        
                        # ã‚­ãƒ¼æ­£è¦åŒ–
                        key_map = {
                            'åºƒå‘Šä¸»': 'advertiser',
                            'å—è³': 'award',
                            'æ¥­ç¨®': 'industry',
                            'åª’ä½“': 'media_type',
                            'æ²è¼‰å¹´åº¦': 'publication_year',
                            'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                            'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                            'åºƒå‘Šä¼šç¤¾': 'agency',
                            'åˆ¶ä½œä¼šç¤¾': 'production_company'
                        }
                        
                        normalized_key = key_map.get(key, key.lower().replace(' ', '_'))
                        if value:
                            data[normalized_key] = value
            
            # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ãƒªãƒ³ã‚¯
            cw_links = soup.find_all('a', href=lambda x: x and '/copitan/' in x)
            if cw_links:
                data['copywriter_links'] = []
                for link in cw_links:
                    cw_data = {'name': link.get_text(strip=True), 'url': link.get('href', '')}
                    id_match = re.search(r'/id/(\d+)', link.get('href', ''))
                    if id_match:
                        cw_data['id'] = id_match.group(1)
                    data['copywriter_links'].append(cw_data)
            
            return data
            
        except Exception as e:
            return {'error': str(e), 'url': url}
    
    def process_all_urls(self, urls):
        """å…¨URLã‚’å‡¦ç†"""
        self.log(f"ğŸ”„ Processing {len(urls)} URLs...")
        
        for i, url in enumerate(urls, 1):
            data = self.extract_detail_data(url)
            self.all_data.append(data)
            
            if 'error' not in data:
                self.processed += 1
            else:
                self.failed += 1
            
            # é€²æ—è¡¨ç¤º
            if i % 100 == 0:
                success_rate = self.processed / i * 100
                self.log(f"ğŸ“Š {i}/{len(urls)} ({i/len(urls)*100:.1f}%) - Success: {success_rate:.1f}%")
                
                # ãƒãƒƒãƒä¿å­˜
                self.save_batch(i)
            
            time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
        
        self.log(f"âœ… Processing complete: {self.processed} success, {self.failed} failed")
    
    def save_batch(self, count):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        filename = f"tcc_final_data/batch_{count:06d}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        self.log(f"ğŸ’¾ Batch saved: {filename}")
    
    def save_final_data(self):
        """æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Œå…¨JSON
        json_file = f"tcc_final_data/tcc_complete_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        
        # CSV
        csv_file = f"tcc_final_data/tcc_complete_{timestamp}.csv"
        self.save_as_csv(csv_file)
        
        # çµ±è¨ˆ
        stats_file = f"tcc_final_data/tcc_stats_{timestamp}.txt"
        self.save_statistics(stats_file)
        
        self.log(f"ğŸ’¾ Final data saved:")
        self.log(f"  ğŸ“‹ JSON: {json_file}")
        self.log(f"  ğŸ“Š CSV: {csv_file}")
        self.log(f"  ğŸ“ˆ Stats: {stats_file}")
    
    def save_as_csv(self, filename):
        """CSVå½¢å¼ã§ä¿å­˜"""
        import csv
        
        if not self.all_data:
            return
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿
        valid_data = [item for item in self.all_data if 'error' not in item]
        if not valid_data:
            return
        
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åé›†
        all_fields = set()
        for item in valid_data:
            all_fields.update(item.keys())
        
        # è¤‡é›‘ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
        simple_fields = [f for f in sorted(all_fields) if f not in ['copywriter_links']]
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=simple_fields)
            writer.writeheader()
            
            for item in valid_data:
                row = {}
                for field in simple_fields:
                    value = item.get(field, '')
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    row[field] = str(value)[:500]  # é•·ã•åˆ¶é™
                writer.writerow(row)
    
    def save_statistics(self, filename):
        """çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜"""
        valid_data = [item for item in self.all_data if 'error' not in item]
        
        stats = []
        stats.append(f"TCC ã‚³ãƒ”ãƒ© å®Œå…¨ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
        stats.append(f"=" * 50)
        stats.append(f"å‡¦ç†æ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        stats.append(f"ç·ä»¶æ•°: {len(self.all_data):,}")
        stats.append(f"æˆåŠŸ: {len(valid_data):,}")
        stats.append(f"å¤±æ•—: {len(self.all_data) - len(valid_data):,}")
        stats.append(f"æˆåŠŸç‡: {len(valid_data)/len(self.all_data)*100:.1f}%")
        stats.append("")
        
        # å¹´åº¦åˆ¥çµ±è¨ˆ
        years = {}
        for item in valid_data:
            year = item.get('publication_year', 'Unknown')
            years[year] = years.get(year, 0) + 1
        
        stats.append("å¹´åº¦åˆ¥çµ±è¨ˆ:")
        for year, count in sorted(years.items(), key=lambda x: x[0], reverse=True)[:20]:
            stats.append(f"  {year}: {count:,}ä»¶")
        
        # åª’ä½“åˆ¥çµ±è¨ˆ
        media = {}
        for item in valid_data:
            media_type = item.get('media_type', 'Unknown')
            media[media_type] = media.get(media_type, 0) + 1
        
        stats.append("\nåª’ä½“åˆ¥çµ±è¨ˆ:")
        for media_type, count in sorted(media.items(), key=lambda x: x[1], reverse=True):
            stats.append(f"  {media_type}: {count:,}ä»¶")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('\n'.join(stats))
    
    def run_complete_crawl(self):
        """å®Œå…¨ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        self.log("ğŸš€ TCC Complete Data Crawler Starting...")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: URLç™ºè¦‹
        urls = self.find_all_detail_urls()
        
        if not urls:
            self.log("âŒ No URLs found!")
            return
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        self.process_all_urls(urls)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚ä¿å­˜
        self.save_final_data()
        
        self.log("ğŸ‰ Complete crawl finished!")

if __name__ == "__main__":
    crawler = DirectTCCCrawler()
    crawler.run_complete_crawl()