import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
import os
from urllib.parse import urljoin, urlparse, parse_qs
import signal
import sys
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue

class CompleteTCCCrawler:
    def __init__(self, output_dir="tcc_complete_data", delay=0.3, max_workers=2):
        self.output_dir = output_dir
        self.delay = delay
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
        })
        
        self.total_items = 0
        self.processed_items = 0
        self.failed_items = 0
        self.start_time = None
        self.stop_crawling = False
        
        # é€²æ—è¿½è·¡
        self.all_urls = set()
        self.processed_urls = set()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¸­æ–­å‡¦ç†
        signal.signal(signal.SIGINT, self.signal_handler)
        
        # çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«
        self.state_file = os.path.join(self.output_dir, "crawler_state.json")
        self.data_file = os.path.join(self.output_dir, "complete_data.json")
        self.batch_counter = 0
    
    def signal_handler(self, signum, frame):
        print(f"\nâš ï¸ åœæ­¢ä¿¡å·ã‚’å—ä¿¡ã€‚å®‰å…¨ã«çµ‚äº†ä¸­...")
        self.stop_crawling = True
        self.save_state()
    
    def save_state(self):
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜"""
        state = {
            'total_items': self.total_items,
            'processed_items': self.processed_items,
            'failed_items': self.failed_items,
            'all_urls': list(self.all_urls),
            'processed_urls': list(self.processed_urls),
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'last_update': datetime.now().isoformat()
        }
        
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çŠ¶æ…‹ä¿å­˜: {len(self.processed_urls)}/{len(self.all_urls)} å®Œäº†")
    
    def load_state(self):
        """ä¿å­˜ã•ã‚ŒãŸçŠ¶æ…‹ã‚’å¾©å…ƒ"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                
                self.total_items = state.get('total_items', 0)
                self.processed_items = state.get('processed_items', 0)
                self.failed_items = state.get('failed_items', 0)
                self.all_urls = set(state.get('all_urls', []))
                self.processed_urls = set(state.get('processed_urls', []))
                
                start_time_str = state.get('start_time')
                if start_time_str:
                    self.start_time = datetime.fromisoformat(start_time_str)
                
                print(f"ğŸ“Š çŠ¶æ…‹å¾©å…ƒ: {len(self.processed_urls)}/{len(self.all_urls)} å®Œäº†")
                return True
            except Exception as e:
                print(f"âš ï¸ çŠ¶æ…‹å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    def fetch_page_robust(self, url, retries=3):
        """å …ç‰¢ãªãƒšãƒ¼ã‚¸å–å¾—"""
        for attempt in range(retries):
            try:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦connection poolingã‚’æ´»ç”¨
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt == retries - 1:
                    print(f"âŒ æœ€çµ‚çš„ã«å¤±æ•—: {url}")
                    return None
                print(f"âš ï¸ ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{retries}: {url}")
                time.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
        return None
    
    def discover_all_pagination_patterns(self):
        """å…¨ã¦ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹"""
        print("ğŸ” ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ§‹é€ ã®å®Œå…¨èª¿æŸ»...")
        
        base_url = "https://www.tcc.gr.jp/copira/"
        params_base = {
            'copy': '',
            'copywriter': '',
            'ad': '',
            'biz': '',
            'media': '',
            'start': '1960',
            'end': '2025',
            'target_prize': 'all'
        }
        
        # ç•°ãªã‚‹è¡¨ç¤ºä»¶æ•°è¨­å®šã‚’è©¦ã™
        items_per_page_options = [10, 15, 20, 50]
        max_discovered_page = 0
        best_pattern = None
        
        for items_per_page in items_per_page_options:
            print(f"ğŸ“Š {items_per_page}ä»¶/ãƒšãƒ¼ã‚¸è¨­å®šã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            
            # æœ€åˆã®ãƒšãƒ¼ã‚¸ã§ç·ä»¶æ•°ã¨æ§‹é€ ã‚’ç¢ºèª
            url = f"{base_url}?" + "&".join([f"{k}={v}" for k, v in params_base.items()]) + f"&limit={items_per_page}"
            html = self.fetch_page_robust(url)
            
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                
                # ç·ä»¶æ•°ã‚’å–å¾—
                result_text = soup.find(string=re.compile(r'\d+ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ'))
                if result_text:
                    match = re.search(r'(\d+)ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ', result_text)
                    if match:
                        self.total_items = int(match.group(1))
                        print(f"  ğŸ“ˆ ç·ä»¶æ•°: {self.total_items:,}ä»¶")
                
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’åˆ†æ
                pagination_links = soup.find_all('a', href=True)
                page_numbers = []
                
                for link in pagination_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # æ•°å­—ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    if text.isdigit():
                        page_numbers.append(int(text))
                    
                    # URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æŠ½å‡º
                    page_match = re.search(r'[?&](?:page|p)=(\d+)', href)
                    if page_match:
                        page_numbers.append(int(page_match.group(1)))
                    
                    # page/123/ ãƒ‘ã‚¿ãƒ¼ãƒ³
                    path_match = re.search(r'/page/(\d+)/', href)
                    if path_match:
                        page_numbers.append(int(path_match.group(1)))
                
                if page_numbers:
                    discovered_max = max(page_numbers)
                    print(f"  ğŸ“„ ç™ºè¦‹æœ€å¤§ãƒšãƒ¼ã‚¸: {discovered_max}")
                    
                    if discovered_max > max_discovered_page:
                        max_discovered_page = discovered_max
                        best_pattern = {
                            'items_per_page': items_per_page,
                            'max_page': discovered_max,
                            'total_estimated': discovered_max * items_per_page
                        }
                
                # æ¨å®šæœ€å¤§ãƒšãƒ¼ã‚¸æ•°ã‚‚è¨ˆç®—
                if self.total_items > 0:
                    estimated_pages = (self.total_items + items_per_page - 1) // items_per_page
                    print(f"  ğŸ§® æ¨å®šãƒšãƒ¼ã‚¸æ•°: {estimated_pages}")
                    
                    if estimated_pages > max_discovered_page:
                        max_discovered_page = estimated_pages
                        best_pattern = {
                            'items_per_page': items_per_page,
                            'max_page': estimated_pages,
                            'total_estimated': self.total_items
                        }
        
        if best_pattern:
            print(f"âœ… æœ€é©ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹:")
            print(f"  - {best_pattern['items_per_page']}ä»¶/ãƒšãƒ¼ã‚¸")
            print(f"  - æœ€å¤§ãƒšãƒ¼ã‚¸: {best_pattern['max_page']:,}")
            print(f"  - æ¨å®šç·ä»¶æ•°: {best_pattern['total_estimated']:,}")
        
        return best_pattern
    
    def extract_all_urls_comprehensive(self):
        """åŒ…æ‹¬çš„URLæŠ½å‡º"""
        print("ğŸŒ å…¨URLæŠ½å‡ºé–‹å§‹ (åŒ…æ‹¬çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)")
        
        # æ—¢å­˜ã®URLã‚’ãƒ­ãƒ¼ãƒ‰
        if self.all_urls:
            print(f"ğŸ“ æ—¢å­˜URL: {len(self.all_urls)}ä»¶")
        
        # æœ€é©ãªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’ç™ºè¦‹
        best_pattern = self.discover_all_pagination_patterns()
        
        if not best_pattern:
            print("âŒ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã®ç™ºè¦‹ã«å¤±æ•—")
            return False
        
        base_url = "https://www.tcc.gr.jp/copira/"
        items_per_page = best_pattern['items_per_page']
        max_page = best_pattern['max_page']
        
        # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
        url_patterns = [
            lambda p: f"{base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit={items_per_page}&page={p}",
            lambda p: f"{base_url}page/{p}/?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit={items_per_page}",
            lambda p: f"{base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&p={p}&limit={items_per_page}"
        ]
        
        print(f"ğŸ”„ {max_page:,}ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ä¸­...")
        
        urls_found = 0
        consecutive_failures = 0
        max_consecutive_failures = 10
        
        for page_num in range(1, max_page + 1):
            if self.stop_crawling:
                break
            
            if page_num % 100 == 0:
                print(f"ğŸ“Š é€²æ—: {page_num:,}/{max_page:,} ({page_num/max_page*100:.1f}%) - ç™ºè¦‹URL: {urls_found:,}")
                self.save_state()
            
            page_urls = []
            
            # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            for pattern_idx, pattern in enumerate(url_patterns):
                if page_urls:  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    break
                
                url = pattern(page_num)
                html = self.fetch_page_robust(url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # è©³ç´°ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                    links = soup.find_all('a', href=True)
                    for link in links:
                        href = link.get('href', '')
                        if '/copira/id/' in href:
                            if not href.startswith('http'):
                                href = f"https://www.tcc.gr.jp{href}"
                            page_urls.append(href)
                    
                    if page_urls:
                        consecutive_failures = 0
                    else:
                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ãŒãƒªãƒ³ã‚¯ãŒãªã„å ´åˆ
                        if len(soup.get_text()) > 1000:  # æœ‰æ„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹
                            consecutive_failures = 0
                        else:
                            consecutive_failures += 1
            
            if page_urls:
                new_urls = [url for url in page_urls if url not in self.all_urls]
                self.all_urls.update(new_urls)
                urls_found += len(new_urls)
                
                if page_num % 50 == 0:
                    print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num}: {len(page_urls)}ä»¶ (æ–°è¦: {len(new_urls)}ä»¶)")
            else:
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    print(f"âš ï¸ é€£ç¶šå¤±æ•— {consecutive_failures}å› - æ—©æœŸçµ‚äº†")
                    break
            
            time.sleep(self.delay)
        
        print(f"âœ… URLæŠ½å‡ºå®Œäº†: {len(self.all_urls):,}ä»¶")
        return True
    
    def parse_detail_page_comprehensive(self, url):
        """åŒ…æ‹¬çš„è©³ç´°ãƒšãƒ¼ã‚¸è§£æ"""
        html = self.fetch_page_robust(url)
        if not html:
            return {'error': 'fetch_failed', 'url': url}
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data = {
                'url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            # IDã‚’æŠ½å‡º
            id_match = re.search(r'/id/(\d+)', url)
            if id_match:
                data['id'] = id_match.group(1)
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # ã€Œ| æ±äº¬ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã‚ºã‚¯ãƒ©ãƒ–ã€ãªã©ã‚’é™¤å»
                title_text = re.sub(r'\s*[|ï½œ]\s*æ±äº¬ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã‚ºã‚¯ãƒ©ãƒ–.*$', '', title_text)
                data['title'] = title_text
            
            # ãƒ¡ã‚¤ãƒ³æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
            info_tables = soup.find_all('table')
            for table in info_tables:
                if table.find('th') and table.find('td'):  # ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹é€ ã‚’ç¢ºèª
                    rows = table.find_all('tr')
                    for row in rows:
                        th = row.find('th')
                        td = row.find('td')
                        if th and td:
                            key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                            value = td.get_text(strip=True)
                            
                            # æ­£è¦åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°
                            key_mapping = {
                                'åºƒå‘Šä¸»': 'advertiser',
                                'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ': 'advertiser',
                                'å—è³': 'award',
                                'æ¥­ç¨®': 'industry',
                                'åª’ä½“': 'media_type',
                                'æ²è¼‰å¹´åº¦': 'publication_year',
                                'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                                'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                                'åºƒå‘Šä¼šç¤¾': 'agency',
                                'åˆ¶ä½œä¼šç¤¾': 'production_company',
                                'ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼': 'director',
                                'ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼': 'producer',
                                'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³': 'campaign',
                                'ä½œå“': 'work_title',
                                'ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼': 'creative_director',
                                'ã‚¢ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼': 'art_director'
                            }
                            
                            normalized_key = key_mapping.get(key, key.lower().replace(' ', '_').replace('ã€€', '_'))
                            if value and value.strip():
                                data[normalized_key] = value
            
            # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ãƒªãƒ³ã‚¯æƒ…å ±
            copywriter_links = soup.find_all('a', href=lambda x: x and '/copitan/' in x)
            if copywriter_links:
                data['copywriter_details'] = []
                for link in copywriter_links:
                    cw_info = {
                        'name': link.get_text(strip=True),
                        'url': link.get('href', '')
                    }
                    # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼IDã‚’æŠ½å‡º
                    id_match = re.search(r'/id/(\d+)', link.get('href', ''))
                    if id_match:
                        cw_info['id'] = id_match.group(1)
                    data['copywriter_details'].append(cw_info)
            
            # å¹´åº¦ã‚’æ­£è¦åŒ–
            if 'publication_year' in data:
                year_match = re.search(r'(\d{4})', data['publication_year'])
                if year_match:
                    data['year'] = int(year_match.group(1))
            
            # ç”»åƒæƒ…å ±
            images = soup.find_all('img', src=True)
            image_urls = []
            for img in images:
                src = img.get('src', '')
                if src and not src.startswith('data:') and 'icon' not in src.lower() and 'logo' not in src.lower():
                    if not src.startswith('http'):
                        src = urljoin(url, src)
                    image_urls.append({
                        'src': src,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', '')
                    })
            
            if image_urls:
                data['images'] = image_urls
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            main_content = soup.find('main') or soup.find('div', class_='main') or soup.find('article')
            if main_content:
                # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã‚’é™¤å»
                for unwanted in main_content.find_all(['nav', 'header', 'footer', 'script', 'style']):
                    unwanted.decompose()
                
                content_text = main_content.get_text(separator='\n', strip=True)
                # é•·ã™ãã‚‹å ´åˆã¯åˆ¶é™
                if len(content_text) > 3000:
                    content_text = content_text[:3000] + "..."
                data['content'] = content_text
            
            return data
            
        except Exception as e:
            return {'error': str(e), 'url': url}
    
    def process_urls_parallel(self):
        """ä¸¦åˆ—ã§URLã‚’å‡¦ç†"""
        remaining_urls = [url for url in self.all_urls if url not in self.processed_urls]
        
        if not remaining_urls:
            print("âœ… å…¨URLãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™")
            return []
        
        print(f"ğŸ”„ {len(remaining_urls):,}ä»¶ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å–å¾—ä¸­...")
        
        all_data = []
        processed_count = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æœªæ¥ã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            future_to_url = {
                executor.submit(self.parse_detail_page_comprehensive, url): url 
                for url in remaining_urls[:1000]  # æœ€åˆã®1000ä»¶ã‹ã‚‰é–‹å§‹
            }
            
            for future in as_completed(future_to_url):
                if self.stop_crawling:
                    break
                
                url = future_to_url[future]
                try:
                    result = future.result()
                    all_data.append(result)
                    self.processed_urls.add(url)
                    
                    if 'error' not in result:
                        self.processed_items += 1
                    else:
                        self.failed_items += 1
                    
                    processed_count += 1
                    
                    # é€²æ—è¡¨ç¤º
                    if processed_count % 50 == 0:
                        elapsed = datetime.now() - self.start_time if self.start_time else timedelta(0)
                        rate = processed_count / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
                        
                        print(f"ğŸ“Š {processed_count:,}/{len(remaining_urls):,} "
                              f"({processed_count/len(remaining_urls)*100:.1f}%) "
                              f"é€Ÿåº¦: {rate:.1f}ä»¶/ç§’")
                    
                    # ãƒãƒƒãƒä¿å­˜
                    if processed_count % 100 == 0:
                        self.save_batch_data(all_data[-100:])
                        self.save_state()
                    
                except Exception as e:
                    print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                    self.failed_items += 1
                
                time.sleep(self.delay)
        
        return all_data
    
    def save_batch_data(self, batch_data):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        self.batch_counter += 1
        batch_file = os.path.join(self.output_dir, f"batch_{self.batch_counter:04d}.json")
        
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, ensure_ascii=False, indent=2)
    
    def consolidate_all_batches(self):
        """å…¨ãƒãƒƒãƒã‚’çµ±åˆ"""
        print("ğŸ”„ ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
        
        all_data = []
        batch_files = [f for f in os.listdir(self.output_dir) if f.startswith('batch_') and f.endswith('.json')]
        batch_files.sort()
        
        for batch_file in batch_files:
            file_path = os.path.join(self.output_dir, batch_file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                    all_data.extend(batch_data)
                print(f"  ğŸ“ çµ±åˆ: {batch_file} ({len(batch_data)}ä»¶)")
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {batch_file} - {e}")
        
        return all_data
    
    def save_final_complete_dataset(self, data):
        """æœ€çµ‚å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        complete_file = os.path.join(self.output_dir, f"tcc_complete_dataset_{timestamp}.json")
        with open(complete_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        summary_file = os.path.join(self.output_dir, f"tcc_summary_{timestamp}.json")
        summary = self.create_comprehensive_summary(data)
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # CSVå½¢å¼
        csv_file = os.path.join(self.output_dir, f"tcc_complete_{timestamp}.csv")
        self.save_as_comprehensive_csv(data, csv_file)
        
        print(f"\nğŸ’¾ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜:")
        print(f"  ğŸ“‹ å®Œå…¨JSON: {complete_file}")
        print(f"  ğŸ“Š çµ±è¨ˆJSON: {summary_file}")
        print(f"  ğŸ“ˆ CSV: {csv_file}")
        
        return complete_file, summary_file, csv_file
    
    def create_comprehensive_summary(self, data):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        valid_data = [item for item in data if 'error' not in item]
        
        summary = {
            'metadata': {
                'total_items': len(data),
                'valid_items': len(valid_data),
                'error_items': len(data) - len(valid_data),
                'success_rate': len(valid_data) / len(data) * 100 if data else 0,
                'crawl_date': datetime.now().isoformat()
            },
            'statistics': {}
        }
        
        # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®çµ±è¨ˆ
        fields_to_analyze = [
            'publication_year', 'media_type', 'industry', 'award', 
            'advertiser', 'agency', 'production_company'
        ]
        
        for field in fields_to_analyze:
            field_stats = {}
            for item in valid_data:
                value = item.get(field, 'Unknown')
                if value and str(value).strip():
                    field_stats[str(value)] = field_stats.get(str(value), 0) + 1
            
            if field_stats:
                summary['statistics'][field] = dict(
                    sorted(field_stats.items(), key=lambda x: x[1], reverse=True)
                )
        
        return summary
    
    def save_as_comprehensive_csv(self, data, csv_file):
        """åŒ…æ‹¬çš„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        import csv
        
        if not data:
            return
        
        # å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åé›†
        all_fields = set()
        for item in data:
            all_fields.update(item.keys())
        
        # è¤‡é›‘ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
        simple_fields = [field for field in sorted(all_fields) 
                        if field not in ['copywriter_details', 'images', 'content']]
        
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=simple_fields)
            writer.writeheader()
            
            for item in data:
                row = {}
                for field in simple_fields:
                    value = item.get(field, '')
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    row[field] = str(value)[:1000]  # é•·ã™ãã‚‹å€¤ã‚’åˆ¶é™
                writer.writerow(row)
    
    def run_complete_extraction(self):
        """å®Œå…¨æŠ½å‡ºã‚’å®Ÿè¡Œ"""
        print("ğŸš€ TCC ã‚³ãƒ”ãƒ© å®Œå…¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹")
        print("=" * 60)
        
        # çŠ¶æ…‹å¾©å…ƒ
        resumed = self.load_state()
        if not resumed:
            self.start_time = datetime.now()
        
        try:
            # ãƒ•ã‚§ãƒ¼ã‚º1: å…¨URLæŠ½å‡º
            if not self.all_urls:
                success = self.extract_all_urls_comprehensive()
                if not success:
                    print("âŒ URLæŠ½å‡ºã«å¤±æ•—")
                    return
            
            print(f"âœ… URLæŠ½å‡ºå®Œäº†: {len(self.all_urls):,}ä»¶")
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—
            all_data = self.process_urls_parallel()
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: ãƒãƒƒãƒçµ±åˆ
            if not all_data:
                all_data = self.consolidate_all_batches()
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: æœ€çµ‚ä¿å­˜
            if all_data:
                files = self.save_final_complete_dataset(all_data)
                self.print_final_summary(all_data)
                print(f"ğŸ‰ å…¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†! ãƒ•ã‚¡ã‚¤ãƒ«: {files[0]}")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
            self.save_state()
        except Exception as e:
            print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            self.save_state()
        finally:
            self.save_state()
    
    def print_final_summary(self, data):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        elapsed = datetime.now() - self.start_time if self.start_time else timedelta(0)
        valid_data = [item for item in data if 'error' not in item]
        
        print(f"\nğŸ‰ å®Œå…¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†!")
        print("=" * 60)
        print(f"ğŸ“Š ç·å–å¾—ä»¶æ•°: {len(data):,}")
        print(f"âœ… æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(valid_data):,}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {len(data) - len(valid_data):,}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {len(valid_data)/len(data)*100:.1f}%")
        print(f"â±ï¸ ç·æ‰€è¦æ™‚é–“: {elapsed}")
        if elapsed.total_seconds() > 0:
            print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(data)/elapsed.total_seconds():.2f}ä»¶/ç§’")
        print("=" * 60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("TCC ã‚³ãƒ”ãƒ© å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ v3.0")
    print("=" * 50)
    print("ğŸ¯ 37,259ä»¶ã®å®Œå…¨å–å¾—ã‚’ç›®æŒ‡ã—ã¾ã™")
    print("âš¡ é«˜é€Ÿãƒ»ä¸¦åˆ—å‡¦ç†å¯¾å¿œ")
    print("ğŸ›¡ï¸ å …ç‰¢æ€§ãƒ»å¾©æ—§æ©Ÿèƒ½ä»˜ã")
    print("ğŸ›‘ Ctrl+C ã§å®‰å…¨ã«ä¸­æ–­ã§ãã¾ã™")
    print("")
    
    crawler = CompleteTCCCrawler(
        output_dir="tcc_complete_data",
        delay=0.3,  # 0.3ç§’é–“éš”ï¼ˆé«˜é€ŸåŒ–ï¼‰
        max_workers=2  # ä¸¦åˆ—å‡¦ç†
    )
    
    crawler.run_complete_extraction()

if __name__ == "__main__":
    main()