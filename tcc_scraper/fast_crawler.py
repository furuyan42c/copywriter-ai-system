#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os
import sys

class FastTCCCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://www.tcc.gr.jp/copira/"
        self.all_urls = set()
        self.all_data = []
        self.processed = 0
        self.failed = 0
        
        # å³åº§ã«å‡ºåŠ›ã™ã‚‹ãŸã‚ã®è¨­å®š
        sys.stdout.flush()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs("tcc_fast_data", exist_ok=True)
        
        print("ğŸš€ Fast TCC Crawler Starting...")
        print(f"ğŸ“… Start time: {datetime.now().strftime('%H:%M:%S')}")
        print("=" * 50)
    
    def get_page(self, url, timeout=15):
        """é«˜é€Ÿãƒšãƒ¼ã‚¸å–å¾—"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except:
            return None
    
    def discover_pagination_urls(self):
        """ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³URLã‚’ç™ºè¦‹"""
        print("ğŸ” Discovering pagination URLs...")
        
        # åŸºæœ¬URL
        base_params = "?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all"
        
        # æœ€åˆã®ãƒšãƒ¼ã‚¸ã§ç·ä»¶æ•°ã‚’ç¢ºèª
        first_url = f"{self.base_url}{base_params}&limit=20"
        html = self.get_page(first_url)
        
        if not html:
            print("âŒ Failed to get first page")
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç·ä»¶æ•°ã‚’å–å¾—
        result_text = soup.get_text()
        total_match = re.search(r'(\d{1,3}(?:,\d{3})*)ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ', result_text)
        if total_match:
            total_str = total_match.group(1).replace(',', '')
            total_items = int(total_str)
            print(f"ğŸ“Š Total items found: {total_items:,}")
        else:
            total_items = 37259  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print(f"ğŸ“Š Using estimated total: {total_items:,}")
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        urls = []
        
        # è¤‡æ•°ã®æˆ¦ç•¥ã§URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        patterns = [
            lambda p: f"{self.base_url}{base_params}&page={p}&limit=20",
            lambda p: f"{self.base_url}page/{p}/{base_params}&limit=20",
            lambda p: f"{self.base_url}{base_params}&p={p}&limit=20"
        ]
        
        # æ¨å®šãƒšãƒ¼ã‚¸æ•°
        estimated_pages = (total_items + 19) // 20  # 20ä»¶ãšã¤ã€åˆ‡ã‚Šä¸Šã’
        print(f"ğŸ“„ Estimated pages: {estimated_pages:,}")
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æœ€åˆã®æ•°ãƒšãƒ¼ã‚¸ã‚’ãƒ†ã‚¹ãƒˆ
        working_pattern = None
        for i, pattern in enumerate(patterns):
            print(f"ğŸ”„ Testing pattern {i+1}...")
            test_url = pattern(2)  # 2ãƒšãƒ¼ã‚¸ç›®ã‚’ãƒ†ã‚¹ãƒˆ
            if self.get_page(test_url):
                working_pattern = pattern
                print(f"âœ… Pattern {i+1} works!")
                break
        
        if not working_pattern:
            print("âŒ No working pagination pattern found")
            return []
        
        # å‹•ä½œã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§URLã‚’ç”Ÿæˆ
        for page in range(1, min(estimated_pages + 1, 2000)):  # å®‰å…¨ä¸Šé™
            urls.append(working_pattern(page))
        
        print(f"âœ… Generated {len(urls):,} pagination URLs")
        return urls
    
    def extract_detail_urls_from_page(self, page_url):
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°URLã‚’æŠ½å‡º"""
        html = self.get_page(page_url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        detail_urls = []
        
        # è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if '/copira/id/' in href:
                if not href.startswith('http'):
                    href = f"https://www.tcc.gr.jp{href}"
                detail_urls.append(href)
        
        return detail_urls
    
    def collect_all_detail_urls(self):
        """å…¨è©³ç´°URLã‚’åé›†"""
        print("ğŸ“¥ Collecting all detail URLs...")
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³URLã‚’å–å¾—
        pagination_urls = self.discover_pagination_urls()
        
        if not pagination_urls:
            return []
        
        # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°URLã‚’æŠ½å‡º
        batch_size = 10
        total_found = 0
        
        for i in range(0, len(pagination_urls), batch_size):
            batch = pagination_urls[i:i+batch_size]
            
            print(f"ğŸ“„ Processing pages {i+1}-{min(i+batch_size, len(pagination_urls))}...", end=' ')
            
            batch_urls = set()
            for page_url in batch:
                detail_urls = self.extract_detail_urls_from_page(page_url)
                batch_urls.update(detail_urls)
                time.sleep(0.1)  # è»½ã„ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            new_urls = batch_urls - self.all_urls
            self.all_urls.update(new_urls)
            total_found += len(new_urls)
            
            print(f"Found {len(new_urls)} new URLs (Total: {len(self.all_urls):,})")
            
            # å®šæœŸçš„ã«ä¿å­˜
            if i % 100 == 0 and self.all_urls:
                self.save_urls_checkpoint()
        
        print(f"âœ… Total detail URLs collected: {len(self.all_urls):,}")
        return list(self.all_urls)
    
    def save_urls_checkpoint(self):
        """URLã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        urls_file = "tcc_fast_data/all_urls.json"
        with open(urls_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.all_urls), f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ URLs saved: {urls_file}")
    
    def parse_detail_page(self, url):
        """è©³ç´°ãƒšãƒ¼ã‚¸ã‚’è§£æ"""
        html = self.get_page(url)
        if not html:
            return {'error': 'fetch_failed', 'url': url}
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data = {
                'url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            # IDæŠ½å‡º
            id_match = re.search(r'/id/(\d+)', url)
            if id_match:
                data['id'] = id_match.group(1)
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            title = soup.find('h1')
            if title:
                data['title'] = title.get_text(strip=True)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    if th and td:
                        key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                        value = td.get_text(strip=True)
                        
                        # ã‚­ãƒ¼æ­£è¦åŒ–
                        key_map = {
                            'åºƒå‘Šä¸»': 'advertiser',
                            'å—è³': 'award',
                            'æ¥­ç¨®': 'industry',
                            'åª’ä½“': 'media_type',
                            'æ²è¼‰å¹´åº¦': 'publication_year',
                            'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                            'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                            'åºƒå‘Šä¼šç¤¾': 'agency',
                            'åˆ¶ä½œä¼šç¤¾': 'production_company'
                        }
                        
                        normalized_key = key_map.get(key, key.lower().replace(' ', '_'))
                        if value:
                            data[normalized_key] = value
            
            # å¹´åº¦ã‚’æ•°å€¤åŒ–
            if 'publication_year' in data:
                year_match = re.search(r'(\d{4})', data['publication_year'])
                if year_match:
                    data['year'] = int(year_match.group(1))
            
            return data
            
        except Exception as e:
            return {'error': str(e), 'url': url}
    
    def process_urls_fast(self, urls):
        """é«˜é€ŸURLå‡¦ç†"""
        print(f"âš¡ Fast processing {len(urls):,} URLs...")
        
        start_time = datetime.now()
        
        for i, url in enumerate(urls, 1):
            data = self.parse_detail_page(url)
            self.all_data.append(data)
            
            if 'error' not in data:
                self.processed += 1
            else:
                self.failed += 1
            
            # é€²æ—è¡¨ç¤º
            if i % 50 == 0:
                elapsed = datetime.now() - start_time
                rate = i / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
                eta_seconds = (len(urls) - i) / rate if rate > 0 else 0
                eta_minutes = eta_seconds / 60
                
                success_rate = self.processed / i * 100
                print(f"ğŸ“Š {i:,}/{len(urls):,} ({i/len(urls)*100:.1f}%) - "
                      f"Success: {success_rate:.1f}% - "
                      f"Speed: {rate:.1f}/s - "
                      f"ETA: {eta_minutes:.1f}min")
                
                # ãƒãƒƒãƒä¿å­˜
                if i % 100 == 0:
                    self.save_batch(i)
            
            time.sleep(0.3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        
        print(f"âœ… Processing complete!")
        print(f"   Success: {self.processed:,}")
        print(f"   Failed: {self.failed:,}")
        print(f"   Total time: {datetime.now() - start_time}")
    
    def save_batch(self, count):
        """ãƒãƒƒãƒä¿å­˜"""
        filename = f"tcc_fast_data/batch_{count:06d}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Batch saved: {filename}")
    
    def save_final_complete_data(self):
        """æœ€çµ‚å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON
        json_file = f"tcc_fast_data/tcc_complete_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        
        # CSV
        csv_file = f"tcc_fast_data/tcc_complete_{timestamp}.csv"
        self.save_as_csv(csv_file)
        
        # çµ±è¨ˆ
        stats_file = f"tcc_fast_data/tcc_stats_{timestamp}.txt"
        self.save_stats(stats_file)
        
        print(f"ğŸ’¾ Final data saved:")
        print(f"   ğŸ“‹ JSON: {json_file}")
        print(f"   ğŸ“Š CSV: {csv_file}")
        print(f"   ğŸ“ˆ Stats: {stats_file}")
        
        return json_file, csv_file, stats_file
    
    def save_as_csv(self, filename):
        """CSVä¿å­˜"""
        import csv
        
        valid_data = [item for item in self.all_data if 'error' not in item]
        if not valid_data:
            return
        
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åé›†
        all_fields = set()
        for item in valid_data:
            all_fields.update(item.keys())
        
        simple_fields = sorted([f for f in all_fields if f != 'copywriter_links'])
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=simple_fields)
            writer.writeheader()
            
            for item in valid_data:
                row = {}
                for field in simple_fields:
                    value = item.get(field, '')
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    row[field] = str(value)[:500]
                writer.writerow(row)
    
    def save_stats(self, filename):
        """çµ±è¨ˆä¿å­˜"""
        valid_data = [item for item in self.all_data if 'error' not in item]
        
        stats = []
        stats.append("TCC ã‚³ãƒ”ãƒ© é«˜é€Ÿã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµæœ")
        stats.append("=" * 50)
        stats.append(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        stats.append(f"ç·ä»¶æ•°: {len(self.all_data):,}")
        stats.append(f"æˆåŠŸ: {len(valid_data):,}")
        stats.append(f"å¤±æ•—: {len(self.all_data) - len(valid_data):,}")
        stats.append(f"æˆåŠŸç‡: {len(valid_data)/len(self.all_data)*100:.1f}%" if self.all_data else "N/A")
        stats.append("")
        
        # å¹´åº¦çµ±è¨ˆ
        years = {}
        for item in valid_data:
            year = item.get('year', 'Unknown')
            years[year] = years.get(year, 0) + 1
        
        stats.append("å¹´åº¦åˆ¥çµ±è¨ˆ (ä¸Šä½20):")
        for year, count in sorted(years.items(), key=lambda x: x[0] if isinstance(x[0], int) else 0, reverse=True)[:20]:
            stats.append(f"  {year}: {count:,}ä»¶")
        
        # åª’ä½“çµ±è¨ˆ
        media = {}
        for item in valid_data:
            media_type = item.get('media_type', 'Unknown')
            media[media_type] = media.get(media_type, 0) + 1
        
        stats.append("\nåª’ä½“åˆ¥çµ±è¨ˆ:")
        for media_type, count in sorted(media.items(), key=lambda x: x[1], reverse=True):
            stats.append(f"  {media_type}: {count:,}ä»¶")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('\n'.join(stats))
    
    def run_fast_crawl(self):
        """é«˜é€Ÿã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: URLåé›†
            urls = self.collect_all_detail_urls()
            
            if not urls:
                print("âŒ No URLs collected!")
                return
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            self.process_urls_fast(urls)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚ä¿å­˜
            files = self.save_final_complete_data()
            
            print("\nğŸ‰ Fast crawl completed successfully!")
            print(f"ğŸ“Š Final stats: {self.processed:,} success, {self.failed:,} failed")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ Interrupted by user")
            if self.all_data:
                self.save_final_complete_data()
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            if self.all_data:
                self.save_final_complete_data()

if __name__ == "__main__":
    crawler = FastTCCCrawler()
    crawler.run_fast_crawl()