#!/usr/bin/env python3
"""
TCC ãƒ‡ãƒ¼ã‚¿çµ±åˆå™¨ - æœ€çµ‚ç‰ˆ
å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã¨è©³ç´°åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
"""
import json
import csv
from datetime import datetime
import sys
from collections import defaultdict

class DataMerger:
    def __init__(self):
        self.merged_data = []
        self.stats = defaultdict(int)
        
    def log(self, message):
        """ãƒ­ã‚°å‡ºåŠ›ï¼ˆå³åº§ã«è¡¨ç¤ºï¼‰"""
        print(message)
        sys.stdout.flush()
    
    def load_original_data(self, original_file):
        """å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        self.log(f"ğŸ“‚ å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {original_file}")
        
        with open(original_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        self.log(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(original_data):,}ä»¶")
        return original_data
    
    def load_classified_data(self, classified_file):
        """è©³ç´°åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        self.log(f"ğŸ“‚ åˆ†é¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {classified_file}")
        
        with open(classified_file, 'r', encoding='utf-8') as f:
            classified_data = json.load(f)
        
        self.log(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(classified_data):,}ä»¶")
        return classified_data
    
    def create_tcc_id_mapping(self, classified_data):
        """TCC IDã«ã‚ˆã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’ä½œæˆ"""
        self.log("ğŸ”— TCC IDãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆä¸­...")
        
        mapping = {}
        for item in classified_data:
            tcc_id = item.get('tcc_id')
            if tcc_id:
                mapping[tcc_id] = item
        
        self.log(f"   ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {len(mapping):,}ä»¶")
        return mapping
    
    def merge_data(self, original_data, classified_mapping):
        """ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        self.log("ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        merged_count = 0
        unmatched_count = 0
        
        for original_item in original_data:
            # å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
            merged_item = original_item.copy()
            
            # URLã‹ã‚‰TCC IDã‚’æŠ½å‡º
            url = original_item.get('url', '')
            tcc_id = None
            
            if '/copira/id/' in url:
                try:
                    tcc_id = int(url.split('/copira/id/')[1].split('/')[0])
                except (ValueError, IndexError):
                    pass
            
            # åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒƒãƒãƒ³ã‚°
            if tcc_id and tcc_id in classified_mapping:
                classified_item = classified_mapping[tcc_id]
                
                # è©³ç´°åˆ†é¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
                merged_item.update({
                    'main_headline': classified_item.get('main_headline'),
                    'sub_headline': classified_item.get('sub_headline'),
                    'body_copy': classified_item.get('body_copy'),
                    'dialogue': classified_item.get('dialogue'),
                    'tagline': classified_item.get('tagline'),
                    'product_info_classified': classified_item.get('product_info'),
                    'notes_classified': classified_item.get('notes'),
                    'raw_copy_text_classified': classified_item.get('raw_copy_text')
                })
                
                merged_count += 1
                
                # çµ±è¨ˆæ›´æ–°
                for field in ['main_headline', 'sub_headline', 'body_copy', 'dialogue', 'tagline', 'product_info', 'notes']:
                    if classified_item.get(field):
                        self.stats[field] += 1
                        
            else:
                unmatched_count += 1
                # åˆ†é¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
                merged_item.update({
                    'main_headline': None,
                    'sub_headline': None,
                    'body_copy': None,
                    'dialogue': None,
                    'tagline': None,
                    'product_info_classified': None,
                    'notes_classified': None,
                    'raw_copy_text_classified': None
                })
            
            self.merged_data.append(merged_item)
        
        self.log(f"âœ… çµ±åˆå®Œäº†:")
        self.log(f"   ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.merged_data):,}ä»¶")
        self.log(f"   åˆ†é¡æ¸ˆã¿: {merged_count:,}ä»¶")
        self.log(f"   æœªåˆ†é¡: {unmatched_count:,}ä»¶")
        
        return self.merged_data
    
    def save_merged_data(self, merged_data):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        self.log("ğŸ’¾ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        json_file = f"tcc_complete_merged_dataset_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        csv_file = f"tcc_complete_merged_dataset_{timestamp}.csv"
        if merged_data:
            # å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åé›†
            all_fields = set()
            for item in merged_data:
                all_fields.update(item.keys())
            
            fieldnames = sorted(all_fields)
            
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for item in merged_data:
                    row = {}
                    for field in fieldnames:
                        value = item.get(field, '')
                        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ¶é™
                        if isinstance(value, str) and len(value) > 8000:
                            value = value[:8000] + '...'
                        row[field] = value
                    writer.writerow(row)
        
        self.log(f"ğŸ’¾ ä¿å­˜å®Œäº†:")
        self.log(f"   ğŸ“Š JSON: {json_file}")
        self.log(f"   ğŸ“‹ CSV: {csv_file}")
        
        return json_file, csv_file
    
    def run_merge(self, original_file, classified_file):
        """çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ"""
        try:
            self.log("ğŸš€ TCC ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹")
            self.log("=" * 60)
            self.log("ç›®æ¨™: å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ + è©³ç´°åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ")
            self.log("=" * 60)
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            original_data = self.load_original_data(original_file)
            classified_data = self.load_classified_data(classified_file)
            
            # ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
            classified_mapping = self.create_tcc_id_mapping(classified_data)
            
            # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            merged_data = self.merge_data(original_data, classified_mapping)
            
            # çµæœä¿å­˜
            files = self.save_merged_data(merged_data)
            
            # çµ±è¨ˆè¡¨ç¤º
            self.log(f"\nğŸ“Š æœ€çµ‚çµ±åˆçµæœ:")
            total = len(merged_data)
            for classification, count in sorted(self.stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total * 100) if total > 0 else 0
                self.log(f"   {classification:15s}: {count:6,d}ä»¶ ({percentage:5.1f}%)")
            
            self.log(f"\nğŸ‰ çµ±åˆå®Œäº†: {len(merged_data):,}ä»¶ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
            self.log("=" * 60)
            
        except Exception as e:
            self.log(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ”„ TCC ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ„ãƒ¼ãƒ«")
    print("ğŸ“‚ å…ƒãƒ‡ãƒ¼ã‚¿ã¨è©³ç´°åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¾ã™")
    print("")
    
    merger = DataMerger()
    # ä½¿ç”¨ä¾‹: merger.run_merge(original_file, classified_file)