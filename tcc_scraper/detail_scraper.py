import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os

def fetch_detail_page(detail_url):
    """è©³ç´°ãƒšãƒ¼ã‚¸ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if not detail_url.startswith('http'):
        detail_url = f"https://www.tcc.gr.jp{detail_url}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
    }
    
    try:
        response = requests.get(detail_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        detail_data = {}
        
        # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = soup.find('h1')
        if title_elem:
            detail_data['page_title'] = title_elem.get_text(strip=True)
        
        # ãƒ¡ã‚¿æƒ…å ±ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
        info_table = soup.find('table', class_='table1__table')
        if info_table:
            rows = info_table.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                    value = td.get_text(strip=True)
                    
                    # ã‚­ãƒ¼ã‚’æ­£è¦åŒ–
                    if 'åºƒå‘Šä¸»' in key or 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ' in key:
                        detail_data['advertiser'] = value
                    elif 'å—è³' in key:
                        detail_data['award'] = value
                    elif 'æ¥­ç¨®' in key:
                        detail_data['industry'] = value
                    elif 'åª’ä½“' in key:
                        detail_data['media_type'] = value
                    elif 'æ²è¼‰å¹´åº¦' in key:
                        detail_data['publication_year'] = value
                    elif 'æ²è¼‰ãƒšãƒ¼ã‚¸' in key:
                        detail_data['page_number'] = value
                    elif 'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼' in key:
                        detail_data['copywriter'] = value
                    elif 'åºƒå‘Šä¼šç¤¾' in key:
                        detail_data['agency'] = value
                    elif 'åˆ¶ä½œä¼šç¤¾' in key:
                        detail_data['production_company'] = value
                    elif 'ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼' in key:
                        detail_data['director'] = value
                    elif 'ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼' in key:
                        detail_data['producer'] = value
                    else:
                        detail_data[key] = value
        
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ”ãƒ¼å†…å®¹ã‚’æŠ½å‡º
        page_text = soup.get_text(separator='\n', strip=True)
        detail_data['full_text'] = page_text
        
        # ç‰¹å®šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        content_divs = soup.find_all(['div', 'section', 'article'])
        for div in content_divs:
            div_text = div.get_text(strip=True)
            if len(div_text) > 50 and 'ã‚³ãƒ”ãƒ¼' in div_text:
                detail_data['copy_content'] = div_text
                break
        
        # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        copywriter_links = soup.find_all('a', href=lambda x: x and '/copitan/' in x)
        if copywriter_links:
            detail_data['copywriter_links'] = []
            for link in copywriter_links:
                detail_data['copywriter_links'].append({
                    'name': link.get_text(strip=True),
                    'url': link.get('href', '')
                })
        
        # ç”»åƒãŒã‚ã‚Œã°å–å¾—
        images = soup.find_all('img')
        if images:
            detail_data['images'] = []
            for img in images:
                src = img.get('src', '')
                alt = img.get('alt', '')
                if src and not src.startswith('data:') and 'icon' not in src.lower():
                    detail_data['images'].append({
                        'src': src,
                        'alt': alt
                    })
        
        detail_data['scraped_at'] = datetime.now().isoformat()
        detail_data['source_url'] = detail_url
        
        return detail_data
        
    except Exception as e:
        print(f"Error fetching detail page {detail_url}: {e}")
        return {'error': str(e), 'source_url': detail_url}

def enhance_basic_data_with_details(json_file, max_details=50):
    """åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã«è©³ç´°æƒ…å ±ã‚’è¿½åŠ """
    with open(json_file, 'r', encoding='utf-8') as f:
        basic_data = json.load(f)
    
    enhanced_data = []
    
    print(f"è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ä¸­... (æœ€å¤§{max_details}ä»¶)")
    print("=" * 60)
    
    count = 0
    for i, item in enumerate(basic_data, 1):
        if count >= max_details:
            break
        
        detail_url = item.get('detail_url', '')
        if detail_url:
            print(f"[{count+1:2d}/{max_details:2d}] {item.get('title', 'No title')[:50]:50s}")
            
            # è©³ç´°æƒ…å ±ã‚’å–å¾—
            detail_info = fetch_detail_page(detail_url)
            
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã¨è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            enhanced_item = item.copy()
            enhanced_item['detail_info'] = detail_info
            
            enhanced_data.append(enhanced_item)
            count += 1
            
            # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
            time.sleep(1)
        else:
            # è©³ç´°URLãŒãªã„å ´åˆã‚‚ãƒ‡ãƒ¼ã‚¿ã«å«ã‚ã‚‹
            enhanced_data.append(item)
    
    return enhanced_data

def save_enhanced_data(enhanced_data, output_file):
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")

def create_detailed_csv_export(enhanced_data, output_file):
    """è©³ç´°æƒ…å ±ã‚’å«ã‚€CSVã‚’ä½œæˆ"""
    import csv
    
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'id', 'title', 'client', 'media', 'year', 'copywriter_names',
            'advertiser', 'award', 'industry', 'media_type', 'publication_year',
            'page_number', 'agency', 'production_company', 'director', 'producer',
            'copy_content', 'detail_url', 'has_detail_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for item in enhanced_data:
            detail_info = item.get('detail_info', {})
            
            row = {
                'id': item.get('id', ''),
                'title': item.get('title', ''),
                'client': item.get('client', ''),
                'media': item.get('media', ''),
                'year': item.get('year', ''),
                'copywriter_names': ', '.join([cw.get('name', '') for cw in item.get('copywriters', [])]),
                'advertiser': detail_info.get('advertiser', ''),
                'award': detail_info.get('award', ''),
                'industry': detail_info.get('industry', ''),
                'media_type': detail_info.get('media_type', ''),
                'publication_year': detail_info.get('publication_year', ''),
                'page_number': detail_info.get('page_number', ''),
                'agency': detail_info.get('agency', ''),
                'production_company': detail_info.get('production_company', ''),
                'director': detail_info.get('director', ''),
                'producer': detail_info.get('producer', ''),
                'copy_content': detail_info.get('copy_content', '')[:500] if detail_info.get('copy_content') else '',  # 500æ–‡å­—ã¾ã§
                'detail_url': item.get('detail_url', ''),
                'has_detail_info': 'Yes' if detail_info and 'error' not in detail_info else 'No'
            }
            writer.writerow(row)
    
    print(f"âœ… è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {output_file}")

def create_analysis_report(enhanced_data, output_file):
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    lines = []
    lines.append("=" * 80)
    lines.append("TCC ã‚³ãƒ”ãƒ© è©³ç´°ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("=" * 80)
    lines.append(f"ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    lines.append(f"ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(enhanced_data)}ä»¶")
    
    # è©³ç´°æƒ…å ±ã®å–å¾—çŠ¶æ³
    with_details = sum(1 for item in enhanced_data if item.get('detail_info') and 'error' not in item.get('detail_info', {}))
    lines.append(f"è©³ç´°æƒ…å ±å–å¾—æˆåŠŸ: {with_details}ä»¶ ({with_details/len(enhanced_data)*100:.1f}%)")
    lines.append("")
    
    # å—è³æƒ…å ±ã®åˆ†æ
    award_count = {}
    industry_count = {}
    agency_count = {}
    
    for item in enhanced_data:
        detail_info = item.get('detail_info', {})
        if detail_info and 'error' not in detail_info:
            # å—è³
            award = detail_info.get('award', '')
            if award:
                award_count[award] = award_count.get(award, 0) + 1
            
            # æ¥­ç¨®
            industry = detail_info.get('industry', '')
            if industry:
                industry_count[industry] = industry_count.get(industry, 0) + 1
            
            # åºƒå‘Šä¼šç¤¾
            agency = detail_info.get('agency', '')
            if agency:
                agency_count[agency] = agency_count.get(agency, 0) + 1
    
    # å—è³æƒ…å ±
    if award_count:
        lines.append("ğŸ† å—è³æƒ…å ±:")
        lines.append("-" * 40)
        for award, count in sorted(award_count.items(), key=lambda x: x[1], reverse=True):
            lines.append(f"{award:30s}: {count:2d}ä»¶")
        lines.append("")
    
    # æ¥­ç¨®æƒ…å ±
    if industry_count:
        lines.append("ğŸ¢ æ¥­ç¨®åˆ¥åˆ†æ:")
        lines.append("-" * 40)
        for industry, count in sorted(industry_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            lines.append(f"{industry:40s}: {count:2d}ä»¶")
        lines.append("")
    
    # åºƒå‘Šä¼šç¤¾
    if agency_count:
        lines.append("ğŸ¬ åºƒå‘Šä¼šç¤¾åˆ¥åˆ†æ:")
        lines.append("-" * 40)
        for agency, count in sorted(agency_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            lines.append(f"{agency:40s}: {count:2d}ä»¶")
        lines.append("")
    
    lines.append("=" * 80)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("TCC è©³ç´°ãƒ‡ãƒ¼ã‚¿ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ„ãƒ¼ãƒ«")
    print("=" * 60)
    
    # æœ€æ–°ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    json_files = [f for f in os.listdir('.') if f.startswith('tcc_data_') and f.endswith('.json')]
    if not json_files:
        print("âŒ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    latest_json = max(json_files, key=os.path.getctime)
    print(f"ğŸ“ åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {latest_json}")
    
    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    enhanced_data = enhance_basic_data_with_details(latest_json, max_details=20)  # 20ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«
    
    if not enhanced_data:
        print("âŒ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    enhanced_json_file = f"tcc_enhanced_{timestamp}.json"
    save_enhanced_data(enhanced_data, enhanced_json_file)
    
    # è©³ç´°CSVã‚’ä½œæˆ
    detailed_csv_file = f"tcc_detailed_{timestamp}.csv"
    create_detailed_csv_export(enhanced_data, detailed_csv_file)
    
    # åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
    analysis_file = f"tcc_detailed_analysis_{timestamp}.txt"
    create_analysis_report(enhanced_data, analysis_file)
    
    print(f"\nğŸ‰ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
    print(f"â”" * 60)
    print(f"ğŸ“‹ æ‹¡å¼µJSONãƒ•ã‚¡ã‚¤ãƒ«: {enhanced_json_file}")
    print(f"ğŸ“Š è©³ç´°CSVãƒ•ã‚¡ã‚¤ãƒ« : {detailed_csv_file}")
    print(f"ğŸ“ˆ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ   : {analysis_file}")
    print(f"â”" * 60)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
    for filename in [enhanced_json_file, detailed_csv_file, analysis_file]:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            print(f"ğŸ“Š {filename}: {size:,} bytes")

if __name__ == "__main__":
    main()