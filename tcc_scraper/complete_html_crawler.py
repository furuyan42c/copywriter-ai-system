#!/usr/bin/env python3
"""
TCC å®Œå…¨HTMLä¿å­˜ä»˜ãã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ - æœ€çµ‚ç‰ˆ
å…¨37,244ä»¶ã®HTMLãƒ‡ãƒ¼ã‚¿ä¿å­˜ + å®Œå…¨æ§‹é€ è§£æã‚’å®Ÿè¡Œ
"""
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os
import sys
import gzip
import csv

class CompleteHTMLCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Connection': 'keep-alive'
        })
        
        self.processed = 0
        self.failed = 0
        self.saved_html = 0
        self.copy_extracted = 0
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs('complete_html_data', exist_ok=True)
        os.makedirs('complete_parsed_data', exist_ok=True)
        
    def log(self, message):
        """ãƒ­ã‚°å‡ºåŠ›ï¼ˆå³åº§ã«è¡¨ç¤ºï¼‰"""
        print(message)
        sys.stdout.flush()
        
    def get_and_save_html(self, url, timeout=10):
        """HTMLã‚’å–å¾—ã—ã¦ä¿å­˜"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            html_content = response.text
            
            # URLã‹ã‚‰IDã‚’æŠ½å‡º
            id_match = re.search(r'/copira/id/(\d+)', url)
            if id_match:
                tcc_id = id_match.group(1)
                
                # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’åœ§ç¸®ä¿å­˜
                html_file = f"complete_html_data/tcc_{tcc_id}.html.gz"
                with gzip.open(html_file, 'wt', encoding='utf-8') as f:
                    f.write(html_content)
                
                self.saved_html += 1
                return html_content
            
            return html_content
            
        except Exception as e:
            self.log(f"âŒ Error fetching {url}: {e}")
            return None
    
    def extract_comprehensive_data(self, url, html):
        """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data = {'url': url}
            
            # TCC IDã‚’æŠ½å‡º
            id_match = re.search(r'/copira/id/(\d+)', url)
            if id_match:
                data['tcc_id'] = int(id_match.group(1))
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            title_elem = soup.find('h1')
            if title_elem:
                data['title'] = title_elem.get_text(strip=True)
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆï¼ˆblock5-1__catchï¼‰
            catch_elem = soup.find('p', class_='block5-1__catch')
            if catch_elem:
                # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã¨spanéƒ¨åˆ†ã‚’åˆ†é›¢ã—ã¦çµåˆ
                copy_parts = []
                
                # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆspanã‚’é™¤ãï¼‰
                main_text = catch_elem.get_text(strip=True)
                span_elem = catch_elem.find('span')
                if span_elem:
                    span_text = span_elem.get_text(strip=True)
                    main_text = main_text.replace(span_text, '').strip()
                
                if main_text:
                    copy_parts.append(main_text)
                
                # spanã®è©³ç´°ãƒ†ã‚­ã‚¹ãƒˆ
                if span_elem:
                    # brã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
                    for br in span_elem.find_all('br'):
                        br.replace_with('\n')
                    span_text = span_elem.get_text(strip=True)
                    if span_text:
                        copy_parts.append(span_text)
                
                # ã‚³ãƒ”ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                if copy_parts:
                    data['copy_text'] = '\n'.join(copy_parts)
                    self.copy_extracted += 1
            
            # ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆblock5-1__notesï¼‰
            notes_elem = soup.find('p', class_='block5-1__notes')
            if notes_elem:
                notes_text = notes_elem.get_text(strip=True)
                if notes_text:
                    data['subtitle'] = notes_text
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        
                        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã®æ­£è¦åŒ–ã¨ãƒãƒƒãƒ”ãƒ³ã‚°
                        key_mappings = {
                            'åºƒå‘Šä¸»': 'advertiser',
                            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ': 'advertiser', 
                            'Client': 'advertiser',
                            'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                            'Copywriter': 'copywriter',
                            'å¹´åº¦': 'year',
                            'å¹´': 'year',
                            'Year': 'year',
                            'åª’ä½“': 'media_type',
                            'Media': 'media_type',
                            'å—è³': 'award',
                            'è³': 'award',
                            'Award': 'award',
                            'æ¥­ç¨®': 'industry',
                            'Industry': 'industry',
                            'åºƒå‘Šä¼šç¤¾': 'agency',
                            'Agency': 'agency',
                            'ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼': 'director',
                            'Director': 'director',
                            'ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼': 'producer',
                            'Producer': 'producer',
                            'ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼': 'planner',
                            'Planner': 'planner',
                            'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                            'ãƒšãƒ¼ã‚¸': 'page_number'
                        }
                        
                        mapped_key = None
                        for k, v in key_mappings.items():
                            if k in key:
                                mapped_key = v
                                break
                        
                        if mapped_key:
                            if mapped_key == 'year':
                                year_match = re.search(r'(\d{4})', value)
                                if year_match:
                                    data[mapped_key] = int(year_match.group(1))
                            elif mapped_key == 'page_number':
                                page_match = re.search(r'(\d+)', value)
                                if page_match:
                                    data[mapped_key] = int(page_match.group(1))
                            else:
                                data[mapped_key] = value
            
            # NO.ç•ªå·ã‚’æŠ½å‡º
            no_elem = soup.find('p', class_='table1__text')
            if no_elem:
                no_text = no_elem.get_text(strip=True)
                no_match = re.search(r'NO\.(\d+)', no_text)
                if no_match:
                    data['no_number'] = int(no_match.group(1))
            
            # å‡¦ç†æ—¥æ™‚ã‚’è¨˜éŒ²
            data['processed_at'] = datetime.now().isoformat()
            
            return data
            
        except Exception as e:
            return {'error': f'Parse error: {str(e)}', 'url': url, 'processed_at': datetime.now().isoformat()}
    
    def process_all_urls_with_html_saving(self, urls):
        """å…¨URLã®HTMLä¿å­˜ä»˜ããƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        total_urls = len(urls)
        start_time = datetime.now()
        all_data = []
        
        self.log(f"ğŸš€ å®Œå…¨HTMLä¿å­˜ä»˜ããƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
        self.log(f"ğŸ“Š å¯¾è±¡URLæ•°: {total_urls:,}")
        self.log(f"ğŸ’¾ HTMLä¿å­˜å…ˆ: complete_html_data/")
        self.log(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: complete_parsed_data/")
        self.log("")
        
        for i, url in enumerate(urls):
            # HTMLã‚’å–å¾—ãƒ»ä¿å­˜
            html = self.get_and_save_html(url)
            
            if html:
                # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                result = self.extract_comprehensive_data(url, html)
                all_data.append(result)
                
                if 'error' not in result:
                    self.processed += 1
                else:
                    self.failed += 1
            else:
                self.failed += 1
                all_data.append({
                    'error': 'HTMLå–å¾—å¤±æ•—', 
                    'url': url, 
                    'processed_at': datetime.now().isoformat()
                })
            
            # é€²æ—è¡¨ç¤ºï¼ˆ100ä»¶ã”ã¨ï¼‰
            if (i + 1) % 100 == 0:
                elapsed = datetime.now() - start_time
                rate = (i + 1) / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
                eta_seconds = (total_urls - (i + 1)) / rate if rate > 0 else 0
                eta_hours = eta_seconds / 3600
                
                progress_pct = (i + 1) / total_urls * 100
                success_rate = self.processed / (i + 1) * 100
                copy_rate = self.copy_extracted / self.processed * 100 if self.processed > 0 else 0
                
                self.log(f"ğŸ“Š é€²æ—: {i+1:,}/{total_urls:,} ({progress_pct:.1f}%)")
                self.log(f"   æˆåŠŸ: {self.processed:,} | å¤±æ•—: {self.failed:,} | æˆåŠŸç‡: {success_rate:.1f}%")
                self.log(f"   HTMLä¿å­˜: {self.saved_html:,}ä»¶")
                self.log(f"   ã‚³ãƒ”ãƒ¼æŠ½å‡º: {self.copy_extracted:,}ä»¶ ({copy_rate:.1f}%)")
                self.log(f"   é€Ÿåº¦: {rate:.1f}ä»¶/ç§’ | æ¨å®šæ®‹ã‚Šæ™‚é–“: {eta_hours:.1f}æ™‚é–“")
                self.log("")
                
                # ä¸­é–“ä¿å­˜ï¼ˆ1000ä»¶ã”ã¨ï¼‰
                if (i + 1) % 1000 == 0:
                    self.save_interim_data(all_data, i + 1)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
            time.sleep(0.3)
        
        self.log(f"âœ… å‡¦ç†å®Œäº†: {len(all_data):,}ä»¶")
        return all_data
    
    def save_interim_data(self, data, count):
        """ä¸­é–“ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"complete_parsed_data/tcc_complete_interim_{count:06d}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        valid_count = len([item for item in data if 'error' not in item])
        self.log(f"ğŸ’¾ ä¸­é–“ä¿å­˜: {filename} ({valid_count:,}ä»¶ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿)")
    
    def save_final_data(self, data):
        """æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        valid_data = [item for item in data if 'error' not in item]
        error_data = [item for item in data if 'error' in item]
        
        self.log("ğŸ’¾ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
        
        # å®Œå…¨JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        json_file = f"complete_parsed_data/tcc_complete_with_html_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(valid_data, f, ensure_ascii=False, indent=2)
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        csv_file = f"complete_parsed_data/tcc_complete_with_html_{timestamp}.csv"
        if valid_data:
            all_fields = set()
            for item in valid_data:
                all_fields.update(item.keys())
            
            fields = sorted(all_fields)
            
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fields)
                writer.writeheader()
                for item in valid_data:
                    row = {}
                    for field in fields:
                        value = item.get(field, '')
                        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šè©°ã‚
                        row[field] = str(value)[:3000] if isinstance(value, str) else str(value)
                    writer.writerow(row)
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ä¿å­˜
        error_file = f"complete_parsed_data/tcc_complete_errors_{timestamp}.json"
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(error_data, f, ensure_ascii=False, indent=2)
        
        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        stats_file = f"complete_parsed_data/tcc_complete_final_stats_{timestamp}.txt"
        self.save_comprehensive_stats(stats_file, valid_data)
        
        self.log("ğŸ’¾ æœ€çµ‚ä¿å­˜å®Œäº†:")
        self.log(f"   ğŸ“‹ å®Œå…¨JSON: {json_file}")
        self.log(f"   ğŸ“Š CSV: {csv_file}")
        self.log(f"   âŒ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°: {error_file}")
        self.log(f"   ğŸ“ˆ çµ±è¨ˆ: {stats_file}")
        
        return json_file, csv_file, stats_file

if __name__ == "__main__":
    print("ğŸš€ TCC å®Œå…¨HTMLä¿å­˜ä»˜ãã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ - æœ€çµ‚ç‰ˆ")
    print("ğŸ’¾ å…¨HTMLãƒ‡ãƒ¼ã‚¿ä¿å­˜ + å®Œå…¨æ§‹é€ è§£æ")
    print("ğŸ¯ ç›®æ¨™: 37,244ä»¶å®Œå…¨å‡¦ç†")
    print("")
    
    crawler = CompleteHTMLCrawler()
    # ä½¿ç”¨ä¾‹: crawler.run_complete_html_crawl(data_file)