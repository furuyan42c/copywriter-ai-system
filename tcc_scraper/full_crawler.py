import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os
from urllib.parse import urljoin, urlparse
import threading
from queue import Queue
import signal
import sys

class TCCFullCrawler:
    def __init__(self, output_dir="tcc_full_data", delay=1.0, max_workers=3):
        self.output_dir = output_dir
        self.delay = delay
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
        })
        
        self.total_items = 0
        self.crawled_items = 0
        self.failed_items = 0
        self.start_time = None
        self.stop_crawling = False
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¸­æ–­æ™‚ã®å‡¦ç†
        signal.signal(signal.SIGINT, self.signal_handler)
    
    def signal_handler(self, signum, frame):
        print(f"\nâš ï¸ ä¸­æ–­ä¿¡å·ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚å®‰å…¨ã«åœæ­¢ä¸­...")
        self.stop_crawling = True
    
    def fetch_page(self, url, retries=3):
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt == retries - 1:
                    print(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {url} - {e}")
                    return None
                time.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
        return None
    
    def parse_list_page(self, html_content):
        """ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°URLãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html_content, 'html.parser')
        detail_urls = []
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰è©³ç´°ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        table = soup.find('table', class_='table2__table')
        if table:
            links = table.find_all('a', href=True)
            for link in links:
                href = link.get('href', '')
                if '/copira/id/' in href:
                    if not href.startswith('http'):
                        href = f"https://www.tcc.gr.jp{href}"
                    detail_urls.append(href)
        
        return detail_urls
    
    def parse_detail_page(self, html_content, url):
        """è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html_content, 'html.parser')
        data = {
            'url': url,
            'scraped_at': datetime.now().isoformat()
        }
        
        # IDã‚’æŠ½å‡º
        id_match = re.search(r'/id/(\d+)', url)
        if id_match:
            data['id'] = id_match.group(1)
        
        # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = soup.find('h1')
        if title_elem:
            data['title'] = title_elem.get_text(strip=True)
        
        # ãƒ¡ã‚¿æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
        info_table = soup.find('table', class_='table1__table')
        if info_table:
            rows = info_table.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                    value = td.get_text(strip=True)
                    
                    # ã‚­ãƒ¼ã‚’æ­£è¦åŒ–ã—ã¦ä¿å­˜
                    key_mapping = {
                        'åºƒå‘Šä¸»': 'advertiser',
                        'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ': 'advertiser', 
                        'å—è³': 'award',
                        'æ¥­ç¨®': 'industry',
                        'åª’ä½“': 'media_type',
                        'æ²è¼‰å¹´åº¦': 'publication_year',
                        'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                        'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                        'åºƒå‘Šä¼šç¤¾': 'agency',
                        'åˆ¶ä½œä¼šç¤¾': 'production_company',
                        'ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼': 'director',
                        'ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼': 'producer',
                        'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³': 'campaign',
                        'ä½œå“': 'work_title'
                    }
                    
                    normalized_key = key_mapping.get(key, key.lower().replace(' ', '_'))
                    data[normalized_key] = value
        
        # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã®ãƒªãƒ³ã‚¯æƒ…å ±
        copywriter_links = soup.find_all('a', href=lambda x: x and '/copitan/' in x)
        if copywriter_links:
            data['copywriter_links'] = []
            for link in copywriter_links:
                copywriter_info = {
                    'name': link.get_text(strip=True),
                    'url': link.get('href', '')
                }
                # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼IDã‚’æŠ½å‡º
                id_match = re.search(r'/id/(\d+)', link.get('href', ''))
                if id_match:
                    copywriter_info['id'] = id_match.group(1)
                data['copywriter_links'].append(copywriter_info)
        
        # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ”ãƒ¼å†…å®¹ãªã©ï¼‰
        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤å¤–
        main_content = soup.find('main') or soup.find('div', class_='main') or soup
        if main_content:
            # ä¸è¦ãªè¦ç´ ã‚’é™¤å»
            for elem in main_content.find_all(['nav', 'header', 'footer', 'script', 'style']):
                elem.decompose()
            
            content_text = main_content.get_text(separator='\n', strip=True)
            # é•·ã™ãã‚‹å ´åˆã¯åˆ¶é™
            if len(content_text) > 5000:
                content_text = content_text[:5000] + "..."
            data['content_text'] = content_text
        
        # ç”»åƒæƒ…å ±
        images = soup.find_all('img', src=True)
        if images:
            data['images'] = []
            for img in images:
                src = img.get('src', '')
                if src and not src.startswith('data:') and 'icon' not in src.lower():
                    if not src.startswith('http'):
                        src = urljoin(url, src)
                    data['images'].append({
                        'src': src,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', '')
                    })
        
        return data
    
    def get_total_pages(self):
        """ç·ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—"""
        base_url = "https://www.tcc.gr.jp/copira/"
        params = {
            'copy': '',
            'copywriter': '',
            'ad': '',
            'biz': '',
            'media': '',
            'start': '1960',
            'end': '2025',
            'target_prize': 'all',
            'limit': '20'  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Š20ä»¶
        }
        
        url = f"{base_url}?" + "&".join([f"{k}={v}" for k, v in params.items()])
        html = self.fetch_page(url)
        
        if html:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç·ä»¶æ•°ã‚’å–å¾—
            result_text = soup.find(string=re.compile(r'\d+ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ'))
            if result_text:
                match = re.search(r'(\d+)ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ', result_text)
                if match:
                    self.total_items = int(match.group(1))
                    total_pages = (self.total_items + 19) // 20  # 20ä»¶ãšã¤ã€åˆ‡ã‚Šä¸Šã’
                    return total_pages
        
        return 0
    
    def crawl_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°"""
        print("ğŸš€ TCC ã‚³ãƒ”ãƒ© å…¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹")
        print("=" * 60)
        
        # ç·ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—
        total_pages = self.get_total_pages()
        if total_pages == 0:
            print("âŒ ç·ãƒšãƒ¼ã‚¸æ•°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        print(f"ğŸ“Š æ¨å®šç·ä»¶æ•°: {self.total_items:,}ä»¶")
        print(f"ğŸ“„ ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages:,}ãƒšãƒ¼ã‚¸")
        print(f"â±ï¸ æ¨å®šæ‰€è¦æ™‚é–“: {total_pages * self.delay / 60:.1f}åˆ†")
        print(f"ğŸ’¾ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print("")
        
        self.start_time = datetime.now()
        all_detail_urls = []
        
        # æ®µéš1: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°URLã‚’åé›†
        print("ğŸ“¥ æ®µéš1: è©³ç´°URLã®åé›†")
        print("-" * 30)
        
        base_url = "https://www.tcc.gr.jp/copira/"
        
        for page_num in range(1, total_pages + 1):
            if self.stop_crawling:
                break
            
            # URLã‚’æ§‹ç¯‰
            if page_num == 1:
                url = f"{base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit=20"
            else:
                url = f"{base_url}page/{page_num}/?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit=20"
            
            print(f"  ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num:4d}/{total_pages:4d} ã‚’å‡¦ç†ä¸­... ", end='')
            
            html = self.fetch_page(url)
            if html:
                detail_urls = self.parse_list_page(html)
                all_detail_urls.extend(detail_urls)
                print(f"âœ… {len(detail_urls)}ä»¶ã®URLå–å¾—")
            else:
                print("âŒ å–å¾—å¤±æ•—")
                self.failed_items += 1
            
            # é€²æ—ã‚’å®šæœŸçš„ã«ä¿å­˜
            if page_num % 100 == 0:
                self.save_urls_checkpoint(all_detail_urls, page_num)
            
            time.sleep(self.delay)
        
        # URLãƒªã‚¹ãƒˆã‚’ä¿å­˜
        urls_file = os.path.join(self.output_dir, f"all_detail_urls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(urls_file, 'w', encoding='utf-8') as f:
            json.dump(all_detail_urls, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è©³ç´°URLåé›†å®Œäº†: {len(all_detail_urls)}ä»¶")
        print(f"ğŸ’¾ URLãƒªã‚¹ãƒˆä¿å­˜: {urls_file}")
        
        # é‡è¤‡é™¤å»
        unique_urls = list(set(all_detail_urls))
        print(f"ğŸ”„ é‡è¤‡é™¤å»å¾Œ: {len(unique_urls)}ä»¶")
        
        if not unique_urls:
            print("âŒ è©³ç´°URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # æ®µéš2: è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        print(f"\nğŸ“¥ æ®µéš2: è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®å–å¾— ({len(unique_urls)}ä»¶)")
        print("-" * 30)
        
        all_data = []
        
        for i, detail_url in enumerate(unique_urls, 1):
            if self.stop_crawling:
                break
            
            print(f"  ğŸ“„ {i:5d}/{len(unique_urls):5d} {detail_url} ... ", end='')
            
            html = self.fetch_page(detail_url)
            if html:
                try:
                    data = self.parse_detail_page(html, detail_url)
                    all_data.append(data)
                    self.crawled_items += 1
                    print("âœ…")
                except Exception as e:
                    print(f"âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    self.failed_items += 1
            else:
                print("âŒ å–å¾—å¤±æ•—")
                self.failed_items += 1
            
            # é€²æ—ã‚’å®šæœŸçš„ã«ä¿å­˜
            if i % 100 == 0:
                self.save_data_checkpoint(all_data, i)
                self.print_progress(i, len(unique_urls))
            
            time.sleep(self.delay)
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.save_final_data(all_data)
        self.print_final_summary(len(unique_urls))
    
    def save_urls_checkpoint(self, urls, page_num):
        """URLãƒªã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        checkpoint_file = os.path.join(self.output_dir, f"urls_checkpoint_page{page_num}.json")
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(urls, f, ensure_ascii=False, indent=2)
        print(f"    ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file}")
    
    def save_data_checkpoint(self, data, count):
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        checkpoint_file = os.path.join(self.output_dir, f"data_checkpoint_{count}.json")
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"    ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file}")
    
    def save_final_data(self, data):
        """æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONå½¢å¼
        json_file = os.path.join(self.output_dir, f"tcc_full_data_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # CSVå½¢å¼
        csv_file = os.path.join(self.output_dir, f"tcc_full_data_{timestamp}.csv")
        self.save_as_csv(data, csv_file)
        
        print(f"\nğŸ’¾ æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜:")
        print(f"  ğŸ“‹ JSON: {json_file}")
        print(f"  ğŸ“Š CSV:  {csv_file}")
    
    def save_as_csv(self, data, csv_file):
        """ãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ä¿å­˜"""
        import csv
        
        if not data:
            return
        
        # ã™ã¹ã¦ã®ã‚­ãƒ¼ã‚’åé›†
        all_keys = set()
        for item in data:
            all_keys.update(item.keys())
        
        # è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿å‹ã‚’é™¤å¤–
        simple_keys = []
        for key in sorted(all_keys):
            if key not in ['copywriter_links', 'images', 'content_text']:
                simple_keys.append(key)
        
        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=simple_keys)
            writer.writeheader()
            
            for item in data:
                row = {}
                for key in simple_keys:
                    value = item.get(key, '')
                    # è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã¯æ–‡å­—åˆ—åŒ–
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    row[key] = value
                writer.writerow(row)
    
    def print_progress(self, current, total):
        """é€²æ—ã‚’è¡¨ç¤º"""
        elapsed = datetime.now() - self.start_time
        rate = current / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
        eta_seconds = (total - current) / rate if rate > 0 else 0
        eta = str(datetime.timedelta(seconds=int(eta_seconds)))
        
        print(f"    ğŸ“Š é€²æ—: {current}/{total} ({current/total*100:.1f}%) "
              f"å–å¾—é€Ÿåº¦: {rate:.1f}ä»¶/ç§’ ETA: {eta}")
    
    def print_final_summary(self, total_attempted):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        elapsed = datetime.now() - self.start_time
        
        print(f"\nğŸ‰ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†!")
        print("=" * 60)
        print(f"ğŸ“Š ç·è©¦è¡Œä»¶æ•°    : {total_attempted:,}")
        print(f"âœ… æˆåŠŸä»¶æ•°      : {self.crawled_items:,}")
        print(f"âŒ å¤±æ•—ä»¶æ•°      : {self.failed_items:,}")
        print(f"ğŸ“ˆ æˆåŠŸç‡        : {self.crawled_items/total_attempted*100:.1f}%")
        print(f"â±ï¸ æ‰€è¦æ™‚é–“      : {elapsed}")
        print(f"ğŸš€ å¹³å‡å–å¾—é€Ÿåº¦   : {self.crawled_items/elapsed.total_seconds():.2f}ä»¶/ç§’")
        print("=" * 60)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("TCC ã‚³ãƒ”ãƒ© å…¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼")
    print("=" * 40)
    print("æ³¨æ„: ã“ã®å‡¦ç†ã¯éå¸¸ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆæ•°æ™‚é–“ï¼‰")
    print("Ctrl+C ã§å®‰å…¨ã«ä¸­æ–­ã§ãã¾ã™")
    print("")
    
    # è¨­å®š
    crawler = TCCFullCrawler(
        output_dir="tcc_full_data",
        delay=1.0,  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã„é–“éš”
        max_workers=1  # ä»Šå›ã¯å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰
    )
    
    # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹
    crawler.crawl_all_data()

if __name__ == "__main__":
    main()