import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime

def fetch_page(page_num=1, items_per_page=20):
    """æŒ‡å®šãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—"""
    base_url = "https://www.tcc.gr.jp/copira/"
    
    params = {
        'copy': '',
        'copywriter': '',
        'ad': '',
        'biz': '',
        'media': '',
        'start': '1960',
        'end': '2025',
        'target_prize': 'all'
    }
    
    if page_num > 1:
        params['page'] = page_num
    
    # items_per_pageã‚’è¨­å®š
    params['limit'] = items_per_page
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
    }
    
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching page {page_num}: {e}")
        return None

def parse_table_data(html_content):
    """HTMLãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html_content, 'html.parser')
    entries = []
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
    table = soup.find('table', class_='table2__table')
    
    if table:
        tbody = table.find('tbody')
        if tbody:
            rows = tbody.find_all('tr')
            
            for row in rows:
                entry = {}
                
                # å¹´åº¦
                year_th = row.find('th')
                if year_th:
                    year_text = year_th.get_text(strip=True)
                    # ã€Œ2023å¹´ã€ã‹ã‚‰ã€Œ2023ã€ã‚’æŠ½å‡º
                    year_match = re.search(r'(\d{4})', year_text)
                    if year_match:
                        entry['year'] = int(year_match.group(1))
                
                # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®td
                main_td = row.find('td')
                if main_td:
                    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆãƒªãƒ³ã‚¯ä»˜ãï¼‰
                    title_link = main_td.find('a')
                    if title_link:
                        entry['title'] = title_link.get_text(strip=True)
                        entry['detail_url'] = title_link.get('href', '')
                        
                        # IDã‚’æŠ½å‡º
                        id_match = re.search(r'/id/(\d+)', entry['detail_url'])
                        if id_match:
                            entry['id'] = id_match.group(1)
                    
                    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå
                    client_p = main_td.find('p', class_='copira__client')
                    if client_p:
                        entry['client'] = client_p.get_text(strip=True)
                    
                    # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼
                    copywriter_p = main_td.find('p', class_='copira__copywriter')
                    if copywriter_p:
                        # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼åã¨ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
                        copywriters = []
                        
                        # ãƒªãƒ³ã‚¯ä»˜ãã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼
                        for link in copywriter_p.find_all('a'):
                            name = link.get_text(strip=True)
                            copywriter_id = None
                            href = link.get('href', '')
                            id_match = re.search(r'/id/(\d+)', href)
                            if id_match:
                                copywriter_id = id_match.group(1)
                            copywriters.append({
                                'name': name,
                                'id': copywriter_id
                            })
                        
                        # ãƒªãƒ³ã‚¯ãªã—ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å–å¾—
                        full_text = copywriter_p.get_text(strip=True)
                        # ãƒªãƒ³ã‚¯ä»˜ãã®åå‰ã‚’é™¤å»ã—ã¦æ®‹ã‚Šã‚’å–å¾—
                        for cw in copywriters:
                            full_text = full_text.replace(cw['name'], '')
                        
                        # æ®‹ã‚Šã®åå‰ã‚’è¿½åŠ 
                        remaining_names = full_text.strip().split()
                        for name in remaining_names:
                            if name and name not in [cw['name'] for cw in copywriters]:
                                copywriters.append({
                                    'name': name,
                                    'id': None
                                })
                        
                        entry['copywriters'] = copywriters
                
                # åª’ä½“
                media_td = row.find_all('td')
                if len(media_td) >= 2:
                    entry['media'] = media_td[-1].get_text(strip=True)
                
                if entry:
                    entries.append(entry)
    
    return entries

def fetch_detail_info(detail_url):
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—"""
    if not detail_url.startswith('http'):
        detail_url = f"https://www.tcc.gr.jp{detail_url}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(detail_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        details = {}
        
        # è©³ç´°æƒ…å ±ã‚’æ¢ã™
        info_table = soup.find('table', class_='table1__table')
        if info_table:
            rows = info_table.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text(strip=True)
                    value = td.get_text(strip=True)
                    
                    if 'å—è³' in key:
                        details['award'] = value
                    elif 'æ¥­ç¨®' in key:
                        details['industry'] = value
                    elif 'æ²è¼‰ãƒšãƒ¼ã‚¸' in key:
                        details['page_number'] = value
                    elif 'åºƒå‘Šä¼šç¤¾' in key:
                        details['agency'] = value
                    elif 'åˆ¶ä½œä¼šç¤¾' in key:
                        details['production'] = value
        
        return details
    except Exception as e:
        print(f"Error fetching detail page: {e}")
        return {}

def scrape_tcc_data(max_items=100, fetch_details=False):
    """TCCã‚³ãƒ”ãƒ©ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    all_entries = []
    page = 1
    items_per_page = 20
    
    print(f"Starting to scrape {max_items} items from TCC Copira...")
    print("="*50)
    
    while len(all_entries) < max_items:
        print(f"\nğŸ“„ Fetching page {page}...")
        html = fetch_page(page, items_per_page)
        
        if not html:
            print(f"  âŒ Failed to fetch page {page}")
            break
        
        entries = parse_table_data(html)
        
        if not entries:
            print(f"  âš ï¸ No entries found on page {page}")
            break
        
        print(f"  âœ… Found {len(entries)} entries")
        
        for i, entry in enumerate(entries, 1):
            if len(all_entries) >= max_items:
                break
            
            # è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if fetch_details and entry.get('detail_url'):
                print(f"    Fetching details for entry {i}...")
                details = fetch_detail_info(entry['detail_url'])
                entry.update(details)
                time.sleep(0.5)
            
            all_entries.append(entry)
            
            # é€²æ—è¡¨ç¤º
            copywriter_names = ', '.join([cw['name'] for cw in entry.get('copywriters', [])])
            print(f"  [{len(all_entries):3d}] {entry.get('year', '----')} | {entry.get('media', '---'):5s} | {entry.get('title', 'No title')[:40]:40s} | {copywriter_names[:30]}")
        
        page += 1
        time.sleep(1)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
    
    return all_entries

def analyze_and_save_data(entries):
    """ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ä¿å­˜"""
    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'tcc_data_{len(entries)}items_{timestamp}.json'
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(entries, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Saved {len(entries)} entries to {filename}")
    
    # åˆ†æ
    analysis = {
        'total_count': len(entries),
        'by_media': {},
        'by_year': {},
        'by_client': {},
        'top_copywriters': {},
        'data_completeness': {
            'has_title': 0,
            'has_client': 0,
            'has_copywriter': 0,
            'has_media': 0,
            'has_year': 0
        }
    }
    
    for entry in entries:
        # åª’ä½“åˆ¥
        media = entry.get('media', 'Unknown')
        analysis['by_media'][media] = analysis['by_media'].get(media, 0) + 1
        
        # å¹´åº¦åˆ¥
        year = str(entry.get('year', 'Unknown'))
        analysis['by_year'][year] = analysis['by_year'].get(year, 0) + 1
        
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥
        client = entry.get('client', 'Unknown')
        analysis['by_client'][client] = analysis['by_client'].get(client, 0) + 1
        
        # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼åˆ¥
        for cw in entry.get('copywriters', []):
            name = cw.get('name', 'Unknown')
            analysis['top_copywriters'][name] = analysis['top_copywriters'].get(name, 0) + 1
        
        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§
        if entry.get('title'): analysis['data_completeness']['has_title'] += 1
        if entry.get('client'): analysis['data_completeness']['has_client'] += 1
        if entry.get('copywriters'): analysis['data_completeness']['has_copywriter'] += 1
        if entry.get('media'): analysis['data_completeness']['has_media'] += 1
        if entry.get('year'): analysis['data_completeness']['has_year'] += 1
    
    # åˆ†æçµæœã‚’è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ")
    print("="*60)
    
    print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
    print(f"  ç·ä»¶æ•°: {analysis['total_count']}")
    
    print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§:")
    for key, count in analysis['data_completeness'].items():
        percentage = (count / analysis['total_count'] * 100) if analysis['total_count'] > 0 else 0
        print(f"  {key}: {count}/{analysis['total_count']} ({percentage:.1f}%)")
    
    print(f"\nğŸ“º åª’ä½“åˆ¥é›†è¨ˆ:")
    for media, count in sorted(analysis['by_media'].items(), key=lambda x: x[1], reverse=True)[:10]:
        percentage = (count / analysis['total_count'] * 100)
        print(f"  {media:10s}: {count:3d}ä»¶ ({percentage:5.1f}%)")
    
    print(f"\nğŸ“… å¹´åº¦åˆ¥é›†è¨ˆ (æœ€æ–°10å¹´):")
    for year, count in sorted(analysis['by_year'].items(), key=lambda x: x[0], reverse=True)[:10]:
        if year != 'Unknown':
            percentage = (count / analysis['total_count'] * 100)
            print(f"  {year}å¹´: {count:3d}ä»¶ ({percentage:5.1f}%)")
    
    print(f"\nğŸ¢ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥é›†è¨ˆ (ä¸Šä½10ç¤¾):")
    for client, count in sorted(analysis['by_client'].items(), key=lambda x: x[1], reverse=True)[:10]:
        if client != 'Unknown':
            print(f"  {client[:30]:30s}: {count:2d}ä»¶")
    
    print(f"\nâœï¸ ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼åˆ¥é›†è¨ˆ (ä¸Šä½10å):")
    for copywriter, count in sorted(analysis['top_copywriters'].items(), key=lambda x: x[1], reverse=True)[:10]:
        if copywriter != 'Unknown':
            print(f"  {copywriter:20s}: {count:2d}ä»¶")
    
    # åˆ†æçµæœã‚‚ä¿å­˜
    analysis_filename = f'tcc_analysis_{timestamp}.json'
    with open(analysis_filename, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… åˆ†æçµæœã‚’ {analysis_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return analysis

def create_summary_report(entries, analysis):
    """ã¾ã¨ã‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    report = []
    report.append("# TCC ã‚³ãƒ”ãƒ© ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    report.append(f"\nç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    report.append(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(entries)}ä»¶\n")
    
    report.append("## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦\n")
    report.append("### åª’ä½“åˆ¥ã®åˆ†å¸ƒ")
    report.append("| åª’ä½“ | ä»¶æ•° | å‰²åˆ |")
    report.append("|------|------|------|")
    for media, count in sorted(analysis['by_media'].items(), key=lambda x: x[1], reverse=True):
        percentage = (count / analysis['total_count'] * 100)
        report.append(f"| {media} | {count} | {percentage:.1f}% |")
    
    report.append("\n### å¹´åº¦åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæœ€æ–°5å¹´ï¼‰")
    report.append("| å¹´åº¦ | ä»¶æ•° |")
    report.append("|------|------|")
    for year, count in sorted(analysis['by_year'].items(), key=lambda x: x[0], reverse=True)[:5]:
        if year != 'Unknown':
            report.append(f"| {year} | {count} |")
    
    report.append("\n## ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ã¾ã¨ã‚æ–¹ã®ææ¡ˆ\n")
    report.append("### 1. åª’ä½“åˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
    report.append("- **TVCM**: æ˜ åƒè¡¨ç¾ã¨ã‚³ãƒ”ãƒ¼ã®é–¢ä¿‚æ€§ã‚’åˆ†æ")
    report.append("- **WEB**: ãƒ‡ã‚¸ã‚¿ãƒ«æ™‚ä»£ã®æ–°ã—ã„ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•")
    report.append("- **ãƒã‚¹ã‚¿ãƒ¼**: ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã¨ã‚³ãƒ”ãƒ¼ã®ç›¸ä¹—åŠ¹æœ")
    report.append("- **ãƒ©ã‚¸ã‚ªCM**: éŸ³å£°ã®ã¿ã§ä¼ãˆã‚‹æŠ€è¡“")
    
    report.append("\n### 2. æ™‚ä»£åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
    report.append("- å¹´ä»£ã”ã¨ã®ã‚³ãƒ”ãƒ¼ã®ç‰¹å¾´å¤‰åŒ–")
    report.append("- ç¤¾ä¼šæƒ…å‹¢ã¨ã‚³ãƒ”ãƒ¼ã®é–¢é€£æ€§")
    report.append("- ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®é€²åŒ–ã«ã‚ˆã‚‹è¡¨ç¾ã®å¤‰åŒ–")
    
    report.append("\n### 3. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ¥ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£")
    report.append("- é•·æœŸçš„ãªãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã®åˆ†æ")
    report.append("- æ¥­ç¨®åˆ¥ã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ç‰¹æ€§")
    
    report.append("\n### 4. ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼åˆ¥ä½œå“é›†")
    report.append("- å€‹äººã®ã‚¹ã‚¿ã‚¤ãƒ«ã¨é€²åŒ–ã®è¿½è·¡")
    report.append("- å¸«å¼Ÿé–¢ä¿‚ã‚„ã‚¹ã‚¯ãƒ¼ãƒ«ã®ç³»è­œ")
    
    report.append("\n### 5. å—è³ä½œå“ã®ç‰¹åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
    report.append("- TCCè³å—è³ä½œå“ã®å…±é€šç‚¹åˆ†æ")
    report.append("- å¯©æŸ»åŸºæº–ã®å¤‰é·")
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    report_filename = f'tcc_summary_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"\nğŸ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ {report_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return '\n'.join(report)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # 100ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆè©³ç´°æƒ…å ±ãªã—ã€é«˜é€Ÿï¼‰
    entries = scrape_tcc_data(max_items=100, fetch_details=False)
    
    if entries:
        # ãƒ‡ãƒ¼ã‚¿åˆ†æã¨ä¿å­˜
        analysis = analyze_and_save_data(entries)
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = create_summary_report(entries, analysis)
        
        print("\n" + "="*60)
        print("ğŸ‰ å‡¦ç†å®Œäº†ï¼")
        print("="*60)
        print(f"å–å¾—ãƒ‡ãƒ¼ã‚¿æ•°: {len(entries)}ä»¶")
        print("ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        print("  - JSONãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«")
        print("  - åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«")
        print("  - ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdownï¼‰")
        
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()