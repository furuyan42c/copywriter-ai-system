#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from urllib3.exceptions import ReadTimeoutError
import socket

class UltimateTCCCrawler:
    def __init__(self):
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        self.session.timeout = (10, 15)  # (æ¥ç¶š, èª­ã¿å–ã‚Š)
        
        # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã‚’è¿½åŠ 
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        self.base_url = "https://www.tcc.gr.jp/copira/"
        self.all_urls = set()
        self.all_data = []
        self.processed = 0
        self.failed = 0
        self.lock = threading.Lock()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs("tcc_ultimate_data", exist_ok=True)
        
        print("ğŸš€ Ultimate TCC Crawler Starting...")
        print(f"ğŸ“… Start time: {datetime.now().strftime('%H:%M:%S')}")
        print("ğŸ›¡ï¸ Enhanced with timeout handling and retry mechanisms")
        print("ğŸ¯ Goal: Complete collection of all TCC data")
        print("=" * 60)
    
    def robust_get_page(self, url, max_retries=3):
        """è¶…å …ç‰¢ãªãƒšãƒ¼ã‚¸å–å¾—"""
        for attempt in range(max_retries):
            try:
                # çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§é«˜é€ŸåŒ–
                response = self.session.get(url, timeout=(5, 10))
                response.raise_for_status()
                return response.text
            except (requests.exceptions.Timeout, ReadTimeoutError, socket.timeout) as e:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"â° Timeout {attempt+1}/{max_retries} for {url}, waiting {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"âŒ Final timeout for {url}")
                    return None
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ Request error {attempt+1}/{max_retries} for {url}: {e}")
                    time.sleep(1)
                    continue
                else:
                    print(f"âŒ Final error for {url}: {e}")
                    return None
            except Exception as e:
                print(f"âŒ Unexpected error for {url}: {e}")
                return None
        
        return None
    
    def get_total_items_estimate(self):
        """ç·ä»¶æ•°æ¨å®šï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œï¼‰"""
        print("ğŸ“Š Estimating total items...")
        
        # è¤‡æ•°ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦ã™
        test_urls = [
            f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit=20",
            f"{self.base_url}?target_prize=all&limit=20",
            f"{self.base_url}?limit=20"
        ]
        
        for url in test_urls:
            html = self.robust_get_page(url)
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                text = soup.get_text()
                
                # ä»¶æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                patterns = [
                    r'(\d{1,3}(?:,\d{3})*)ä»¶ãŒæ¤œç´¢ã•ã‚Œã¾ã—ãŸ',
                    r'(\d{1,3}(?:,\d{3})*)ä»¶ã®æ¤œç´¢çµæœ',
                    r'å…¨(\d{1,3}(?:,\d{3})*)ä»¶'
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, text)
                    if match:
                        total_str = match.group(1).replace(',', '')
                        total = int(total_str)
                        print(f"ğŸ“Š Found total: {total:,} items")
                        return total
                
                # HTMLã‹ã‚‰æ¨å®š
                links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                if links:
                    print(f"ğŸ“„ Found {len(links)} detail links on first page")
                    return 37259  # æ—¢çŸ¥ã®æ¨å®šå€¤ã‚’ä½¿ç”¨
        
        print("ğŸ“Š Using conservative estimate: 37,259 items")
        return 37259
    
    def smart_year_search(self):
        """ã‚¹ãƒãƒ¼ãƒˆå¹´åº¦æ¤œç´¢"""
        print("ğŸ“… Starting smart year-by-year search...")
        
        all_urls = set()
        successful_years = 0
        
        # æœ€è¿‘ã®å¹´ã‹ã‚‰é †ç•ªã«æ¤œç´¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒå¤šã„å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
        years = list(range(2025, 1959, -1))
        
        for year in years:
            try:
                print(f"ğŸ“… Year {year}...", end=' ')
                
                year_urls = set()
                base_url = f"{self.base_url}?start={year}&end={year}&target_prize=all&limit=20"
                
                # ãã®å¹´ã®å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—
                page = 1
                empty_pages = 0
                max_empty = 3
                
                while empty_pages < max_empty and page <= 100:  # å®‰å…¨ä¸Šé™
                    page_url = f"{base_url}&page={page}"
                    html = self.robust_get_page(page_url)
                    
                    if html:
                        soup = BeautifulSoup(html, 'html.parser')
                        links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                        
                        page_found = 0
                        for link in links:
                            href = link.get('href', '')
                            if not href.startswith('http'):
                                href = f"https://www.tcc.gr.jp{href}"
                            year_urls.add(href)
                            page_found += 1
                        
                        if page_found > 0:
                            empty_pages = 0
                        else:
                            empty_pages += 1
                    else:
                        empty_pages += 1
                    
                    page += 1
                    time.sleep(0.2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                
                all_urls.update(year_urls)
                if year_urls:
                    successful_years += 1
                print(f"{len(year_urls)} URLs (Total: {len(all_urls):,})")
                
                # é€²æ—ä¿å­˜
                if year % 5 == 0:
                    self.save_checkpoint(all_urls, f"year_{year}")
                
            except Exception as e:
                print(f"Error in year {year}: {e}")
                continue
        
        print(f"âœ… Year search complete: {successful_years} years, {len(all_urls):,} URLs")
        return all_urls
    
    def smart_pagination_search(self):
        """ã‚¹ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œç´¢"""
        print("ğŸ“„ Starting smart pagination search...")
        
        all_urls = set()
        base_url = f"{self.base_url}?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit=20"
        
        page = 1
        consecutive_failures = 0
        max_failures = 5
        
        while consecutive_failures < max_failures and page <= 2000:  # å®‰å…¨ä¸Šé™
            try:
                print(f"ğŸ“„ Page {page}...", end=' ')
                
                # è¤‡æ•°ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
                url_patterns = [
                    f"{base_url}&page={page}",
                    f"{self.base_url}page/{page}/?copy=&copywriter=&ad=&biz=&media=&start=1960&end=2025&target_prize=all&limit=20"
                ]
                
                page_urls = set()
                for url_pattern in url_patterns:
                    html = self.robust_get_page(url_pattern)
                    if html:
                        soup = BeautifulSoup(html, 'html.parser')
                        links = soup.find_all('a', href=lambda x: x and '/copira/id/' in x)
                        
                        for link in links:
                            href = link.get('href', '')
                            if not href.startswith('http'):
                                href = f"https://www.tcc.gr.jp{href}"
                            page_urls.add(href)
                        
                        if page_urls:
                            break  # æˆåŠŸã—ãŸã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                
                if page_urls:
                    all_urls.update(page_urls)
                    consecutive_failures = 0
                    print(f"{len(page_urls)} URLs (Total: {len(all_urls):,})")
                else:
                    consecutive_failures += 1
                    print("No URLs")
                
                # å®šæœŸä¿å­˜
                if page % 50 == 0:
                    self.save_checkpoint(all_urls, f"page_{page}")
                
                page += 1
                time.sleep(0.3)
                
            except Exception as e:
                consecutive_failures += 1
                print(f"Error: {e}")
                continue
        
        print(f"âœ… Pagination search complete: {len(all_urls):,} URLs")
        return all_urls
    
    def save_checkpoint(self, urls, suffix):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        filename = f"tcc_ultimate_data/checkpoint_{suffix}_{datetime.now().strftime('%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(list(urls), f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Checkpoint: {filename}")
    
    def collect_all_urls(self):
        """å…¨URLåé›†ï¼ˆè¤‡æ•°æˆ¦ç•¥ï¼‰"""
        print("ğŸŒ Collecting all URLs with multiple strategies...")
        
        # ç·ä»¶æ•°æ¨å®š
        estimated_total = self.get_total_items_estimate()
        
        # æˆ¦ç•¥1: å¹´åº¦åˆ¥æ¤œç´¢
        year_urls = self.smart_year_search()
        self.all_urls.update(year_urls)
        print(f"ğŸ“… After year search: {len(self.all_urls):,} URLs")
        
        # æˆ¦ç•¥2: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œç´¢
        pagination_urls = self.smart_pagination_search()
        new_pagination_urls = pagination_urls - self.all_urls
        self.all_urls.update(new_pagination_urls)
        print(f"ğŸ“„ After pagination search: +{len(new_pagination_urls):,} new URLs (Total: {len(self.all_urls):,})")
        
        # æœ€çµ‚ä¿å­˜
        self.save_checkpoint(self.all_urls, "final_urls")
        
        coverage = len(self.all_urls) / estimated_total * 100 if estimated_total > 0 else 0
        print(f"ğŸ¯ URL collection complete: {len(self.all_urls):,} / {estimated_total:,} ({coverage:.1f}%)")
        
        return list(self.all_urls)
    
    def parse_detail_page_robust(self, url):
        """è¶…å …ç‰¢ãªè©³ç´°ãƒšãƒ¼ã‚¸è§£æ"""
        html = self.robust_get_page(url)
        if not html:
            return {'error': 'fetch_failed', 'url': url}
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            data = {
                'url': url,
                'scraped_at': datetime.now().isoformat()
            }
            
            # IDæŠ½å‡º
            id_match = re.search(r'/id/(\d+)', url)
            if id_match:
                data['id'] = id_match.group(1)
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            title_selectors = ['h1', 'title', '.page-title', '.entry-title']
            for selector in title_selectors:
                title = soup.select_one(selector)
                if title:
                    title_text = title.get_text(strip=True)
                    if title_text and len(title_text) > 5:  # æœ‰åŠ¹ãªã‚¿ã‚¤ãƒˆãƒ«
                        data['title'] = title_text
                        break
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œï¼‰
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['th', 'td'])
                    if len(cells) >= 2:
                        key_cell = cells[0]
                        value_cell = cells[1]
                        
                        key = key_cell.get_text(strip=True).replace('ï¼š', '').replace(':', '')
                        value = value_cell.get_text(strip=True)
                        
                        if key and value:
                            # ã‚­ãƒ¼æ­£è¦åŒ–ãƒãƒƒãƒ”ãƒ³ã‚°
                            key_map = {
                                'åºƒå‘Šä¸»': 'advertiser',
                                'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ': 'advertiser',
                                'å—è³': 'award',
                                'æ¥­ç¨®': 'industry',
                                'åª’ä½“': 'media_type',
                                'æ²è¼‰å¹´åº¦': 'publication_year',
                                'æ²è¼‰ãƒšãƒ¼ã‚¸': 'page_number',
                                'ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼': 'copywriter',
                                'åºƒå‘Šä¼šç¤¾': 'agency',
                                'åˆ¶ä½œä¼šç¤¾': 'production_company',
                                'ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼': 'director',
                                'ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼': 'producer'
                            }
                            
                            normalized_key = key_map.get(key, key.lower().replace(' ', '_').replace('ã€€', '_'))
                            data[normalized_key] = value
            
            # å¹´åº¦ã‚’æ•°å€¤åŒ–
            year_fields = ['publication_year', 'year']
            for field in year_fields:
                if field in data:
                    year_match = re.search(r'(\d{4})', str(data[field]))
                    if year_match:
                        data['year'] = int(year_match.group(1))
                        break
            
            # ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ãƒªãƒ³ã‚¯
            cw_links = soup.find_all('a', href=lambda x: x and '/copitan/' in x)
            if cw_links:
                data['copywriter_links'] = []
                for link in cw_links:
                    cw_data = {
                        'name': link.get_text(strip=True),
                        'url': link.get('href', '')
                    }
                    id_match = re.search(r'/id/(\d+)', link.get('href', ''))
                    if id_match:
                        cw_data['id'] = id_match.group(1)
                    data['copywriter_links'].append(cw_data)
            
            return data
            
        except Exception as e:
            return {'error': f'parse_error: {str(e)}', 'url': url}
    
    def process_urls_ultimate(self, urls):
        """ç©¶æ¥µã®ä¸¦åˆ—URLå‡¦ç†"""
        print(f"âš¡ Ultimate processing of {len(urls):,} URLs...")
        
        start_time = datetime.now()
        batch_size = 50
        
        # ãƒãƒƒãƒå‡¦ç†ã§é€²æ—ã‚’ç´°ã‹ãç®¡ç†
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(urls) + batch_size - 1) // batch_size
            
            print(f"ğŸ“¦ Batch {batch_num}/{total_batches} ({len(batch_urls)} URLs)...")
            
            # å„ãƒãƒƒãƒã‚’ä¸¦åˆ—å‡¦ç†
            with ThreadPoolExecutor(max_workers=2) as executor:  # æ§ãˆã‚ãªä¸¦åˆ—æ•°
                future_to_url = {
                    executor.submit(self.parse_detail_page_robust, url): url 
                    for url in batch_urls
                }
                
                batch_results = []
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        result = future.result(timeout=30)
                        batch_results.append(result)
                        
                        with self.lock:
                            if 'error' not in result:
                                self.processed += 1
                            else:
                                self.failed += 1
                    
                    except Exception as e:
                        with self.lock:
                            self.failed += 1
                        batch_results.append({'error': f'future_error: {str(e)}', 'url': url})
                
                # ãƒãƒƒãƒçµæœã‚’è¿½åŠ 
                self.all_data.extend(batch_results)
            
            # ãƒãƒƒãƒã”ã¨ã®é€²æ—è¡¨ç¤º
            total_processed = len(self.all_data)
            elapsed = datetime.now() - start_time
            rate = total_processed / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0
            eta_seconds = (len(urls) - total_processed) / rate if rate > 0 else 0
            eta_minutes = eta_seconds / 60
            
            success_rate = self.processed / total_processed * 100 if total_processed > 0 else 0
            
            print(f"ğŸ“Š Progress: {total_processed:,}/{len(urls):,} ({total_processed/len(urls)*100:.1f}%)")
            print(f"   Success: {success_rate:.1f}% | Speed: {rate:.1f}/s | ETA: {eta_minutes:.1f}min")
            
            # å®šæœŸä¿å­˜
            if batch_num % 10 == 0:
                self.save_batch_data(batch_num)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1)
        
        print(f"âœ… Ultimate processing complete!")
        print(f"   Total: {len(self.all_data):,} | Success: {self.processed:,} | Failed: {self.failed:,}")
        print(f"   Duration: {datetime.now() - start_time}")
    
    def save_batch_data(self, batch_num):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        filename = f"tcc_ultimate_data/batch_{batch_num:04d}_{datetime.now().strftime('%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Batch saved: {filename}")
    
    def save_ultimate_data(self):
        """ç©¶æ¥µãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Œå…¨JSON
        json_file = f"tcc_ultimate_data/tcc_ultimate_complete_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.all_data, f, ensure_ascii=False, indent=2)
        
        # æˆåŠŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã®JSON
        valid_data = [item for item in self.all_data if 'error' not in item]
        valid_json_file = f"tcc_ultimate_data/tcc_ultimate_valid_{timestamp}.json"
        with open(valid_json_file, 'w', encoding='utf-8') as f:
            json.dump(valid_data, f, ensure_ascii=False, indent=2)
        
        # CSV
        csv_file = f"tcc_ultimate_data/tcc_ultimate_{timestamp}.csv"
        self.save_as_csv(csv_file, valid_data)
        
        # è©³ç´°çµ±è¨ˆ
        stats_file = f"tcc_ultimate_data/tcc_ultimate_stats_{timestamp}.txt"
        self.save_ultimate_stats(stats_file, valid_data)
        
        print(f"ğŸ’¾ Ultimate data saved:")
        print(f"   ğŸ“‹ Complete JSON: {json_file}")
        print(f"   âœ… Valid JSON: {valid_json_file}")
        print(f"   ğŸ“Š CSV: {csv_file}")
        print(f"   ğŸ“ˆ Stats: {stats_file}")
        
        return json_file, valid_json_file, csv_file, stats_file
    
    def save_as_csv(self, filename, data):
        """CSVä¿å­˜"""
        import csv
        
        if not data:
            return
        
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åé›†
        all_fields = set()
        for item in data:
            all_fields.update(item.keys())
        
        # è¤‡é›‘ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–ã—ã¦ã‚½ãƒ¼ãƒˆ
        simple_fields = sorted([f for f in all_fields if f not in ['copywriter_links']])
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=simple_fields)
            writer.writeheader()
            
            for item in data:
                row = {}
                for field in simple_fields:
                    value = item.get(field, '')
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    row[field] = str(value)[:1000]  # é•·ã•åˆ¶é™
                writer.writerow(row)
    
    def save_ultimate_stats(self, filename, data):
        """ç©¶æ¥µçµ±è¨ˆä¿å­˜"""
        total_data = len(self.all_data)
        valid_data = len(data)
        
        stats = []
        stats.append("TCC ã‚³ãƒ”ãƒ© ç©¶æ¥µã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµæœ")
        stats.append("=" * 60)
        stats.append(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        stats.append(f"ç·ä»¶æ•°: {total_data:,}")
        stats.append(f"æœ‰åŠ¹ä»¶æ•°: {valid_data:,}")
        stats.append(f"ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {total_data - valid_data:,}")
        stats.append(f"æˆåŠŸç‡: {valid_data/total_data*100:.1f}%" if total_data > 0 else "N/A")
        stats.append(f"ç›®æ¨™é”æˆç‡: {valid_data/37259*100:.1f}%")
        stats.append("")
        
        if data:
            # å¹´åº¦çµ±è¨ˆ
            years = {}
            for item in data:
                year = item.get('year', item.get('publication_year', 'Unknown'))
                years[year] = years.get(year, 0) + 1
            
            stats.append("ğŸ“… å¹´åº¦åˆ¥çµ±è¨ˆ (ä¸Šä½30):")
            year_items = sorted(years.items(), key=lambda x: x[0] if isinstance(x[0], int) else 0, reverse=True)
            for year, count in year_items[:30]:
                stats.append(f"  {year}: {count:,}ä»¶")
            
            # åª’ä½“çµ±è¨ˆ
            media_types = {}
            for item in data:
                media = item.get('media_type', 'Unknown')
                media_types[media] = media_types.get(media, 0) + 1
            
            stats.append("\nğŸ“º åª’ä½“åˆ¥çµ±è¨ˆ:")
            for media, count in sorted(media_types.items(), key=lambda x: x[1], reverse=True):
                stats.append(f"  {media}: {count:,}ä»¶")
            
            # å—è³çµ±è¨ˆ
            awards = {}
            for item in data:
                award = item.get('award', 'ãªã—')
                awards[award] = awards.get(award, 0) + 1
            
            stats.append("\nğŸ† å—è³çµ±è¨ˆ (ä¸Šä½20):")
            for award, count in sorted(awards.items(), key=lambda x: x[1], reverse=True)[:20]:
                stats.append(f"  {award}: {count:,}ä»¶")
            
            # åºƒå‘Šä¸»çµ±è¨ˆ
            advertisers = {}
            for item in data:
                advertiser = item.get('advertiser', 'Unknown')
                advertisers[advertiser] = advertisers.get(advertiser, 0) + 1
            
            stats.append("\nğŸ¢ åºƒå‘Šä¸»çµ±è¨ˆ (ä¸Šä½20):")
            for advertiser, count in sorted(advertisers.items(), key=lambda x: x[1], reverse=True)[:20]:
                if advertiser != 'Unknown':
                    stats.append(f"  {advertiser}: {count:,}ä»¶")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('\n'.join(stats))
    
    def run_ultimate_crawl(self):
        """ç©¶æ¥µã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        try:
            print("ğŸ¯ Starting ultimate comprehensive crawl...")
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: å…¨URLåé›†
            urls = self.collect_all_urls()
            
            if not urls:
                print("âŒ No URLs collected!")
                return
            
            print(f"\nâš¡ Starting data extraction for {len(urls):,} URLs...")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            self.process_urls_ultimate(urls)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚ä¿å­˜
            files = self.save_ultimate_data()
            
            print("\nğŸ‰ ULTIMATE CRAWL COMPLETED SUCCESSFULLY!")
            print("=" * 60)
            print(f"ğŸ“Š Final Results:")
            print(f"   Total URLs processed: {len(self.all_data):,}")
            print(f"   Successful extractions: {self.processed:,}")
            print(f"   Failed extractions: {self.failed:,}")
            print(f"   Success rate: {self.processed/len(self.all_data)*100:.1f}%")
            print(f"   Target coverage: {self.processed/37259*100:.1f}%")
            print("=" * 60)
            
        except KeyboardInterrupt:
            print("\nâš ï¸ Interrupted by user - saving current progress...")
            if self.all_data:
                self.save_ultimate_data()
        except Exception as e:
            print(f"\nâŒ Ultimate error: {e}")
            if self.all_data:
                self.save_ultimate_data()

if __name__ == "__main__":
    print("ğŸš€ Starting TCC Ultimate Crawler...")
    print("âš¡ This is the most comprehensive and robust version")
    print("ğŸ›¡ï¸ Enhanced with timeout handling, retry mechanisms, and error recovery")
    print("")
    
    crawler = UltimateTCCCrawler()
    crawler.run_ultimate_crawl()